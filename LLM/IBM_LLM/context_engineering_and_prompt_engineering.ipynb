{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd2c143d",
   "metadata": {},
   "source": [
    "# **In-Context Engineering and Prompt Templates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5aaa436",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --user \"ibm-watsonx-ai==0.2.6\"\n",
    "!pip install --user \"langchain==0.1.16\" \n",
    "!pip install --user \"langchain-ibm==0.1.4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44ff016d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from ibm_watsonx_ai.foundation_models import Model\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes\n",
    "from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7136a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_model(prompt_txt, params=None):\n",
    "    model_id = 'mistralai/mixtral-8x7b-instruct-v01'\n",
    "\n",
    "    default_params = {\n",
    "        \"max_new_tokens\": 256,\n",
    "        \"min_new_tokens\": 0,\n",
    "        \"temperature\": 0.5,\n",
    "        \"top_p\": 0.2,\n",
    "        \"top_k\": 1\n",
    "    }\n",
    "\n",
    "    if params:\n",
    "        default_params.update(params)\n",
    "\n",
    "    parameters = {\n",
    "        GenParams.MAX_NEW_TOKENS: default_params[\"max_new_tokens\"],  # this controls the maximum number of tokens in the generated output\n",
    "        GenParams.MIN_NEW_TOKENS: default_params[\"min_new_tokens\"], # this controls the minimum number of tokens in the generated output\n",
    "        GenParams.TEMPERATURE: default_params[\"temperature\"], # this randomness or creativity of the model's responses\n",
    "        GenParams.TOP_P: default_params[\"top_p\"],\n",
    "        GenParams.TOP_K: default_params[\"top_k\"]\n",
    "    }\n",
    "    \n",
    "    credentials = {\n",
    "        \"url\": \"https://us-south.ml.cloud.ibm.com\"\n",
    "    }\n",
    "    \n",
    "    project_id = \"skills-network\"\n",
    "    \n",
    "    model = Model(\n",
    "        model_id=model_id,\n",
    "        params=parameters,\n",
    "        credentials=credentials,\n",
    "        project_id=project_id\n",
    "    )\n",
    "    \n",
    "    mixtral_llm = WatsonxLLM(model=model)\n",
    "    response  = mixtral_llm.invoke(prompt_txt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a68e08ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decoding_method': 'sample',\n",
       " 'length_penalty': {'decay_factor': 2.5, 'start_index': 5},\n",
       " 'temperature': 0.5,\n",
       " 'top_p': 0.2,\n",
       " 'top_k': 1,\n",
       " 'random_seed': 33,\n",
       " 'repetition_penalty': 2,\n",
       " 'min_new_tokens': 50,\n",
       " 'max_new_tokens': 200,\n",
       " 'stop_sequences': ['fail'],\n",
       " ' time_limit': 600000,\n",
       " 'truncate_input_tokens': 200,\n",
       " 'return_options': {'input_text': True,\n",
       "  'generated_tokens': True,\n",
       "  'input_tokens': True,\n",
       "  'token_logprobs': True,\n",
       "  'token_ranks': False,\n",
       "  'top_n_tokens': False}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GenParams().get_example_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de94555d",
   "metadata": {},
   "source": [
    "### Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e3422f",
   "metadata": {},
   "source": [
    "In this example, let's introduce a basic prompt that utilizes specific parameters to guide the language model's response. You'll then define a simple prompt and retrieve the model's response,\n",
    "\n",
    "The prompt used is \"The wind is\". Let the model generate itself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f13544",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"max_new_tokens\": 128,\n",
    "    \"min_new_tokens\": 10,\n",
    "    \"temperature\": 0.5,\n",
    "    \"top_p\": 0.2,\n",
    "    \"top_k\": 1\n",
    "}\n",
    "\n",
    "prompt = \"The wind is\"\n",
    "\n",
    "response = llm_model(prompt, params)\n",
    "print(f\"prompt: {prompt}\\n\")\n",
    "print(f\"response : {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c6f668",
   "metadata": {},
   "source": [
    "As you can see from the response, the model continues generating content following the initial prompt, \"The wind is\". You might notice that the response appears truncated or incomplete. This is because you have set the `max_new_tokens,` which restricts the number of tokens the model can generate.\n",
    "\n",
    "Try to adjust the parameters and observe the difference in the response.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fdb172",
   "metadata": {},
   "source": [
    "### Zero-shot prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edec11f5",
   "metadata": {},
   "source": [
    "Here is an example of a zero-shot prompt. \n",
    "\n",
    "Zero-shot learning is crucial for testing a model's ability to apply its pre-trained knowledge to new, unseen tasks without additional training. This capability is valuable for gauging the model's generalization skills.\n",
    "\n",
    "In this example, let's demonstrate a zero-shot learning scenario using a prompt that asks the model to classify a statement without any prior specific training on similar tasks. The prompt requests the model to assess the truthfulness of the statement: \"The Eiffel Tower is located in Berlin.\". After defining the prompt, you'll execute it with default parameters and print the response.\n",
    "\n",
    "This approach helps you understand how well the model can handle direct questions based on its underlying knowledge and reasoning abilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0efd533",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Classify the following statement as true or false: \n",
    "            'The Eiffel Tower is located in Berlin.'\n",
    "\n",
    "            Answer:\n",
    "\"\"\"\n",
    "response = llm_model(prompt, params)\n",
    "print(f\"prompt: {prompt}\\n\")\n",
    "print(f\"response : {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bc8637",
   "metadata": {},
   "source": [
    "### One-Shot Prompt\n",
    "\n",
    "Here is a one-shot learning example where the model is given a single example to help guide its translation from English to French.\n",
    "\n",
    "The prompt provides a sample translation pairing, \"How is the weather today?\" translated to \"Comment est le temps aujourd'hui?\" This example serves as a guide for the model to understand the task context and desired format. The model is then tasked with translating a new sentence, \"Where is the nearest supermarket?\" without further guidance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f6be58",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"max_new_tokens\": 20,\n",
    "    \"temperature\": 0.1,\n",
    "}\n",
    "\n",
    "prompt = \"\"\"Here is an example of translating a sentence from English to French:\n",
    "\n",
    "            English: “How is the weather today?”\n",
    "            French: “Comment est le temps aujourd'hui?”\n",
    "            \n",
    "            Now, translate the following sentence from English to French:\n",
    "            \n",
    "            English: “Where is the nearest supermarket?”\n",
    "            \n",
    "\"\"\"\n",
    "response = llm_model(prompt, params)\n",
    "print(f\"prompt: {prompt}\\n\")\n",
    "print(f\"response : {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b04c828",
   "metadata": {},
   "source": [
    "### Few Shot Prompt\n",
    "\n",
    "Here is an example of few-shot learning by classifying emotions from text statements. \n",
    "\n",
    "Let's provide the model with three examples, each labeled with an appropriate emotion—joy, frustration, and sadness—to establish a pattern or guideline on how to categorize emotions in statements.\n",
    "\n",
    "After presenting these examples, let's challenge the model with a new statement: \"That movie was so scary I had to cover my eyes.\" The task for the model is to classify the emotion expressed in this new statement based on the learning from the provided examples. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0a825f",
   "metadata": {},
   "outputs": [],
   "source": [
    " #parameters  `max_new_tokens` to 10, which constrains the model to generate brief responses\n",
    "\n",
    "params = {\n",
    "    \"max_new_tokens\": 10,\n",
    "}\n",
    "\n",
    "prompt = \"\"\"Here are few examples of classifying emotions in statements:\n",
    "\n",
    "            Statement: 'I just won my first marathon!'\n",
    "            Emotion: Joy\n",
    "            \n",
    "            Statement: 'I can't believe I lost my keys again.'\n",
    "            Emotion: Frustration\n",
    "            \n",
    "            Statement: 'My best friend is moving to another country.'\n",
    "            Emotion: Sadness\n",
    "            \n",
    "            Now, classify the emotion in the following statement:\n",
    "            Statement: 'That movie was so scary I had to cover my eyes.’\n",
    "            \n",
    "\n",
    "\"\"\"\n",
    "response = llm_model(prompt, params)\n",
    "print(f\"prompt: {prompt}\\n\")\n",
    "print(f\"response : {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb85f81",
   "metadata": {},
   "source": [
    "### Chain of Thought Prompt\n",
    "\n",
    "Here is an example of the Chain-of-Thought (CoT) prompting technique, designed to guide the model through a sequence of reasoning steps to solve a problem. In this example, the problem is a simple arithmetic question: “A store had 22 apples. They sold 15 apples today and received a new delivery of 8 apples. How many apples are there now?”\n",
    "\n",
    "The CoT technique involves structuring the prompt by instructing the model to “Break down each step of your calculation.” This encourages the model to include explicit reasoning steps, mimicking human-like problem-solving processes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5a7a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"temperature\": 0.5,\n",
    "}\n",
    "\n",
    "prompt = \"\"\"Consider the problem: 'A store had 22 apples. They sold 15 apples today and got a new delivery of 8 apples. \n",
    "            How many apples are there now?’\n",
    "\n",
    "            Break down each step of your calculation\n",
    "\n",
    "\"\"\"\n",
    "response = llm_model(prompt, params)\n",
    "print(f\"prompt: {prompt}\\n\")\n",
    "print(f\"response : {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7cc7ec",
   "metadata": {},
   "source": [
    "### SElf Consistency\n",
    "\n",
    "This example demonstrates the self-consistency technique in reasoning through multiple calculations for a single problem. The problem posed is: “When I was 6, my sister was half my age. Now I am 70, what age is my sister?”\n",
    "\n",
    "The prompt instructs, “Provide three independent calculations and explanations, then determine the most consistent result.” This encourages the model to engage in critical thinking and consistency checking, which are vital for complex decision-making processes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d209bba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"max_new_tokens\": 512,\n",
    "}\n",
    "\n",
    "prompt = \"\"\"When I was 6, my sister was half of my age. Now I am 70, what age is my sister?\n",
    "\n",
    "            Provide three independent calculations and explanations, then determine the most consistent result.\n",
    "\n",
    "\"\"\"\n",
    "response = llm_model(prompt, params)\n",
    "print(f\"prompt: {prompt}\\n\")\n",
    "print(f\"response : {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b733d08c",
   "metadata": {},
   "source": [
    "### Prompt Template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba2cd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'mistralai/mixtral-8x7b-instruct-v01'\n",
    "\n",
    "parameters = {\n",
    "    GenParams.MAX_NEW_TOKENS: 256,  # this controls the maximum number of tokens in the generated output\n",
    "    GenParams.TEMPERATURE: 0.5, # this randomness or creativity of the model's responses\n",
    "}\n",
    "\n",
    "credentials = {\n",
    "    \"url\": \"https://us-south.ml.cloud.ibm.com\"\n",
    "}\n",
    "\n",
    "project_id = \"skills-network\"\n",
    "\n",
    "model = Model(\n",
    "    model_id=model_id,\n",
    "    params=parameters,\n",
    "    credentials=credentials,\n",
    "    project_id=project_id\n",
    ")\n",
    "\n",
    "mixtral_llm = WatsonxLLM(model=model)\n",
    "mixtral_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969e1f48",
   "metadata": {},
   "source": [
    "Use the `PromptTemplate` to create a template for a string-based prompt. In this template, you'll define two parameters: `adjective` and `content`. These parameters allow for the reuse of the prompt across different situations. For instance, to adapt the prompt to various contexts, simply pass the relevant values to these parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07bb06d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['adjective', 'content'], template='Tell me a {adjective} joke about {content}.\\n')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"Tell me a {adjective} joke about {content}.\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936bdcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt.format(adjective=\"funny\", content=\"chickens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9f4b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=mixtral_llm)\n",
    "response = llm_chain.invoke(input = {\"adjective\": \"funny\", \"content\": \"chickens\"})\n",
    "print(response[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4830bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm_chain.invoke(input = {\"adjective\": \"sad\", \"content\": \"fish\"})\n",
    "print(response[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92a0e5b",
   "metadata": {},
   "source": [
    "### Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4323c03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = \"\"\"\n",
    "        The rapid advancement of technology in the 21st century has transformed various industries, including healthcare, education, and transportation. \n",
    "        Innovations such as artificial intelligence, machine learning, and the Internet of Things have revolutionized how we approach everyday tasks and complex problems. \n",
    "        For instance, AI-powered diagnostic tools are improving the accuracy and speed of medical diagnoses, while smart transportation systems are making cities more efficient and reducing traffic congestion. \n",
    "        Moreover, online learning platforms are making education more accessible to people around the world, breaking down geographical and financial barriers. \n",
    "        These technological developments are not only enhancing productivity but also contributing to a more interconnected and informed society.\n",
    "\"\"\"\n",
    "\n",
    "template = \"\"\"Summarize the {content} in one sentence.\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=mixtral_llm)\n",
    "response = llm_chain.invoke(input = {\"content\": content})\n",
    "print(response[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b181644",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
