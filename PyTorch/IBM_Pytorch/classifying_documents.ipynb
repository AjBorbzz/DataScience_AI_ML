{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18e82342",
   "metadata": {},
   "source": [
    "## Project Introduction: Document Classification with Machine Learning\n",
    "\n",
    "In this personal project, I explore how machine learning can be used to automatically classify news articles into relevant categories. This kind of document classification is a key component of intelligent content search engines, helping users find information quickly and accurately.\n",
    "\n",
    "To implement this, I’ll use **PyTorch** and the **torchtext** library to preprocess and prepare raw text data for model training. The pipeline includes converting text into tensors, organizing it into batches, and building a classifier capable of predicting the topic of a given article.\n",
    "\n",
    "This project demonstrates the practical application of natural language processing (NLP) and deep learning in organizing large volumes of unstructured text—a valuable capability for search, recommendation, and information retrieval systems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31633dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b7862b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "class NewsDataset(Dataset):\n",
    "    \"\"\"Custom Dataset class for news articles\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, labels, vocab, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenize and convert to indices\n",
    "        tokens = self.tokenizer(text.lower())\n",
    "        indices = [self.vocab.get(token, self.vocab['<UNK>']) for token in tokens]\n",
    "        \n",
    "        # Pad or truncate to max_length\n",
    "        if len(indices) > self.max_length:\n",
    "            indices = indices[:self.max_length]\n",
    "        else:\n",
    "            indices.extend([self.vocab['<PAD>']] * (self.max_length - len(indices)))\n",
    "            \n",
    "        return torch.tensor(indices, dtype=torch.long), torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e19843",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    \"\"\"Neural network model for text classification\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, n_layers=2, dropout=0.3):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, n_layers, \n",
    "                           dropout=dropout if n_layers > 1 else 0, \n",
    "                           batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # *2 for bidirectional\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_length)\n",
    "        embedded = self.embedding(x)  # (batch_size, seq_length, embed_dim)\n",
    "        \n",
    "        # LSTM layer\n",
    "        lstm_out, (hidden, _) = self.lstm(embedded)\n",
    "        \n",
    "        # Use the last hidden state (forward and backward)\n",
    "        # hidden shape: (n_layers*2, batch_size, hidden_dim)\n",
    "        forward_hidden = hidden[-2]  # Last layer forward\n",
    "        backward_hidden = hidden[-1]  # Last layer backward\n",
    "        \n",
    "        # Concatenate forward and backward hidden states\n",
    "        final_hidden = torch.cat((forward_hidden, backward_hidden), dim=1)\n",
    "        \n",
    "        # Apply dropout and final linear layer\n",
    "        output = self.dropout(final_hidden)\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1243e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsClassifier:\n",
    "    \"\"\"Main class for news article classification\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim=128, hidden_dim=256, max_length=512):\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = get_tokenizer('basic_english')\n",
    "        self.vocab = None\n",
    "        self.label_to_idx = None\n",
    "        self.idx_to_label = None\n",
    "        self.model = None\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "    def build_vocab(self, texts, min_freq=2):\n",
    "        \"\"\"Build vocabulary from training texts\"\"\"\n",
    "        print(\"Building vocabulary...\")\n",
    "        \n",
    "        # Tokenize all texts and count word frequencies\n",
    "        counter = Counter()\n",
    "        for text in texts:\n",
    "            tokens = self.tokenizer(str(text).lower())\n",
    "            counter.update(tokens)\n",
    "        \n",
    "        # Create vocabulary with special tokens\n",
    "        vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "        for word, freq in counter.items():\n",
    "            if freq >= min_freq:\n",
    "                vocab[word] = len(vocab)\n",
    "                \n",
    "        print(f\"Vocabulary size: {len(vocab)}\")\n",
    "        self.vocab = vocab\n",
    "        return vocab\n",
    "\n",
    "\n",
    "    def prepare_labels(self, labels):\n",
    "        \"\"\"Convert labels to numerical indices\"\"\"\n",
    "        unique_labels = sorted(list(set(labels)))\n",
    "        self.label_to_idx = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "        self.idx_to_label = {idx: label for label, idx in self.label_to_idx.items()}\n",
    "        \n",
    "        numerical_labels = [self.label_to_idx[label] for label in labels]\n",
    "        return numerical_labels\n",
    "    \n",
    "    def create_data_loaders(self, train_texts, train_labels, test_texts, test_labels, batch_size=32):\n",
    "        \"\"\"Create PyTorch data loaders\"\"\"\n",
    "        train_dataset = NewsDataset(train_texts, train_labels, self.vocab, \n",
    "                                  self.tokenizer, self.max_length)\n",
    "        test_dataset = NewsDataset(test_texts, test_labels, self.vocab, \n",
    "                                 self.tokenizer, self.max_length)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        return train_loader, test_loader\n",
    "    \n",
    "    def build_model(self, num_classes):\n",
    "        \"\"\"Build the neural network model\"\"\"\n",
    "        vocab_size = len(self.vocab)\n",
    "        self.model = TextClassifier(\n",
    "            vocab_size=vocab_size,\n",
    "            embed_dim=self.embed_dim,\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            output_dim=num_classes\n",
    "        ).to(self.device)\n",
    "        \n",
    "        print(f\"Model built with {sum(p.numel() for p in self.model.parameters())} parameters\")\n",
    "        return self.model\n",
    "\n",
    "\n",
    "    def train(self, train_loader, test_loader, num_epochs=10, learning_rate=0.001):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        print(f\"Training on {self.device}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            # Training phase\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "            correct_predictions = 0\n",
    "            total_samples = 0\n",
    "            \n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                output = self.model(data)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                pred = output.argmax(dim=1)\n",
    "                correct_predictions += (pred == target).sum().item()\n",
    "                total_samples += target.size(0)\n",
    "            \n",
    "            # Calculate training accuracy\n",
    "            train_accuracy = correct_predictions / total_samples\n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            \n",
    "            # Validation phase\n",
    "            test_accuracy = self.evaluate(test_loader)\n",
    "            \n",
    "            print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "            print(f'  Train Loss: {avg_loss:.4f} | Train Acc: {train_accuracy:.4f}')\n",
    "            print(f'  Test Acc: {test_accuracy:.4f}')\n",
    "            print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f0c732",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
