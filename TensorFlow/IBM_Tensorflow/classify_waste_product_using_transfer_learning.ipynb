{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc759e59",
   "metadata": {},
   "source": [
    "## Overview\n",
    "The city of GreenCity has been struggling with waste management, especially in distinguishing between recyclable and organic waste. The local waste management organization, EcoClean, is tasked with improving the efficiency of waste sorting. However, the current manual process is both time-consuming and error-prone. To enhance the system, EcoClean wants you to develop an AI-powered solution that can automatically classify waste products using image recognition techniques. This project aims to build a model that can differentiate between recyclable and organic waste products using transfer learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c57ab0",
   "metadata": {},
   "source": [
    "## Background\n",
    "EcoClean currently lacks an efficient and scalable method to automate the waste sorting process. The manual sorting of waste is not only labor-intensive but also prone to errors, leading to contamination of recyclable materials. The goal of this project is to leverage machine learning and computer vision to automate the classification of waste products, improving efficiency and reducing contamination rates. The project will use transfer learning with a pre-trained VGG16 model to classify images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159a7d26",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "After completing this project, you will be able to:\n",
    "\n",
    "- Apply transfer learning using the VGG16 model for image classification\n",
    "- Prepare and preprocess image data for a machine learning task\n",
    "- Fine-tune a pre-trained model to improve classification accuracy\n",
    "- Evaluate the model's performance using appropriate metrics\n",
    "- Visualize model predictions on test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff1d81c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed tensorboard-2.17.1 tensorflow-2.17.0\n",
      "Successfully installed numpy-1.26.0\n",
      "Successfully installed scikit-learn-1.5.1\n",
      "Successfully installed matplotlib-3.9.2\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.67.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.17.0 | tail -n 1\n",
    "!pip install numpy==1.26.0 | tail -n 1\n",
    "!pip install scikit-learn==1.5.1  | tail -n 1\n",
    "!pip install matplotlib==3.9.2  | tail -n 1\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f558339",
   "metadata": {},
   "source": [
    "### Import required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96c19e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "# import random, shutil\n",
    "import glob\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "from matplotlib.image import imread\n",
    "\n",
    "# from os import makedirs,listdir\n",
    "# from shutil import copyfile\n",
    "# from random import seed\n",
    "# from random import random\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# from tensorflow.keras.layers import Conv2D, MaxPooling2D,GlobalAveragePooling2D, Input\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# from tensorflow.keras.applications import InceptionV3\n",
    "from sklearn import metrics\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfb62c9",
   "metadata": {},
   "source": [
    "## <a id='toc1_5_'></a>[Background](#toc0_)\n",
    "\n",
    "\n",
    "**Transfer learning** uses the concept of keeping the early layers of a pre-trained network, and re-training the later layers on a specific dataset. You can leverage some state of that network on a related task.\n",
    "\n",
    "A typical transfer learning workflow in Keras looks something like this:\n",
    "\n",
    "1.  Initialize base model, and load pre-trained weights (e.g. ImageNet)\n",
    "2.  \"Freeze\" layers in the base model by setting `training = False`\n",
    "3.  Define a new model that goes on top of the output of the base model's layers.\n",
    "4.  Train resulting model on your data set.\n",
    "\n",
    "## <a id='toc1_6_'></a>[Create a model for distinguishing recyclable and organic waste images](#toc0_)\n",
    "\n",
    "### <a id='toc1_6_1_'></a>[Dataset](#toc0_)\n",
    "\n",
    "You will be using the [Waste Classification Dataset](https://www.kaggle.com/datasets/techsash/waste-classification-data?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera35714171-2022-01-01).\n",
    "\n",
    "Your goal is to train an algorithm on these images and to predict the labels for images in your test set (1 = recyclable, 0 = organic).\n",
    "\n",
    "### <a id='toc1_6_2_'></a>[Importing Data](#toc0_)\n",
    "\n",
    "This will create a `o-vs-r-split` directory in your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "506732f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file\n",
      "Extracting file with progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1207/1207 [00:00<00:00, 4988.63file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished extracting file\n",
      "Finished extracting file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "\n",
    "url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/kd6057VPpABQ2FqCbgu9YQ/o-vs-r-split-reduced-1200.zip\"\n",
    "file_name = \"o-vs-r-split-reduced-1200.zip\"\n",
    "\n",
    "print(\"Downloading file\")\n",
    "with requests.get(url, stream=True) as response:\n",
    "    response.raise_for_status()\n",
    "    with open(file_name, 'wb') as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "\n",
    "\n",
    "def extract_file_with_progress(file_name):\n",
    "    print(\"Extracting file with progress\")\n",
    "    with zipfile.ZipFile(file_name, 'r') as zip_ref:\n",
    "        members = zip_ref.infolist() \n",
    "        with tqdm(total=len(members), unit='file') as progress_bar:\n",
    "            for member in members:\n",
    "                zip_ref.extract(member)\n",
    "                progress_bar.update(1)\n",
    "    print(\"Finished extracting file\")\n",
    "\n",
    "\n",
    "extract_file_with_progress(file_name)\n",
    "\n",
    "print(\"Finished extracting file\")\n",
    "os.remove(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f3b47a",
   "metadata": {},
   "source": [
    "### <a id='toc1_6_3_'></a>[Define configuration options](#toc0_)\n",
    "\n",
    "It's time to define some model configuration options.\n",
    "\n",
    "*   **batch size** is set to 32.\n",
    "*   The **number of classes** is 2.\n",
    "*   You will use 20% of the data for **validation** purposes.\n",
    "*   You have two **labels** in your dataset: organic (O), recyclable (R).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aecb6b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows, img_cols = 150, 150\n",
    "batch_size = 32\n",
    "n_epochs = 10\n",
    "n_classes = 2\n",
    "val_split = 0.2\n",
    "verbosity = 1\n",
    "path = 'o-vs-r-split/train/'\n",
    "path_test = 'o-vs-r-split/test/'\n",
    "input_shape = (img_rows, img_cols, 3)\n",
    "labels = ['O', 'R']\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a848232",
   "metadata": {},
   "source": [
    "### <a id='toc1_6_4_'></a>[Loading Images using ImageGeneratorClass](#toc0_)\n",
    "\n",
    "Transfer learning works best when models are trained on smaller datasets. \n",
    "\n",
    "The folder structure looks as follows:\n",
    "\n",
    "```python\n",
    "o-vs-r-split/\n",
    "└── train\n",
    "    ├── O\n",
    "    └── R\n",
    "└── test\n",
    "    ├── O\n",
    "    └── R\n",
    "```\n",
    "\n",
    "\n",
    "#### <a id='toc1_6_4_1_'></a>[ImageDataGenerators](#toc0_)\n",
    "\n",
    "\n",
    "Now you will create ImageDataGenerators used for training, validation and testing.\n",
    "\n",
    "Image data generators create batches of tensor image data with real-time data augmentation. The generators loop over the data in batches and are useful in feeding data to the training process. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b630abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ImageDataGenerators for training and validation and testing\n",
    "train_datagen = ImageDataGenerator(\n",
    "    validation_split = val_split,\n",
    "    rescale=1.0/255.0,\n",
    "\twidth_shift_range=0.1, \n",
    "    height_shift_range=0.1, \n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(\n",
    "    validation_split = val_split,\n",
    "    rescale=1.0/255.0,\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec0afde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 800 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory = path,\n",
    "    seed = seed,\n",
    "    batch_size = batch_size, \n",
    "    class_mode='binary',\n",
    "    shuffle = True,\n",
    "    target_size=(img_rows, img_cols),\n",
    "    subset = 'training'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69fbd8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "val_generator = val_datagen.flow_from_directory(\n",
    "    directory = path,\n",
    "    seed = seed,\n",
    "    batch_size = batch_size, \n",
    "    class_mode='binary',\n",
    "    shuffle = True,\n",
    "    target_size=(img_rows, img_cols),\n",
    "    subset = 'validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c111e8",
   "metadata": {},
   "source": [
    "## Create a test_generator using the test_datagen object\n",
    "- directory should be set to path_test.\n",
    "- class_mode should be set to 'binary'.\n",
    "- seed should be set to seed.\n",
    "- batch_size should be set to batch_size.\n",
    "- shuffle should be set to False.\n",
    "- target_size should be set to (img_rows, img_cols)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3814e908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = test_datagen.flow_from_directory(\n",
    "        directory=path_test,\n",
    "        class_mode='binary',\n",
    "        seed=seed,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        target_size=(img_rows, img_cols)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4865e8e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
