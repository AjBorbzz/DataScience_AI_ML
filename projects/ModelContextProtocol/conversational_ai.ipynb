{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79dee65e",
   "metadata": {},
   "source": [
    "This chatbot example uses Anthropic's Claude model to build a conversation loop. It passes the entire message history each time to maintain a memory of the dialogue. This helps the AI understand what’s going on, what was asked before, and how to respond intelligently—making the conversation feel more natural and coherent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a077561",
   "metadata": {},
   "source": [
    "Maintaining context in conversational AI allows:\n",
    "\n",
    "* Multi-turn memory: Models can reference past questions and answers\n",
    "\n",
    "* Improved relevance: Responses reflect prior inputs\n",
    "\n",
    "* Personalization: The AI adapts to the user's tone and interests\n",
    "\n",
    "* Natural interaction: More human-like conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6b5e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "# Choose your model (must be pulled via `ollama pull`)\n",
    "MODEL_NAME = \"llama3\"\n",
    "\n",
    "# Initialize the conversation history\n",
    "history = []\n",
    "\n",
    "def chat_with_ollama(user_input):\n",
    "    # Append user input to history\n",
    "    history.append({'role': 'user', 'content': user_input})\n",
    "\n",
    "    # Send the history to the model\n",
    "    response = ollama.chat(\n",
    "        model=MODEL_NAME,\n",
    "        messages=history\n",
    "    )\n",
    "\n",
    "    # Extract the model's response\n",
    "    assistant_response = response['message']['content']\n",
    "    print(f\"Ollama: {assistant_response}\\n\")\n",
    "\n",
    "    # Append the response to history\n",
    "    history.append({'role': 'assistant', 'content': assistant_response})\n",
    "\n",
    "# Example chat loop\n",
    "print(\"Start chatting with your local AI! Type 'exit' to quit.\\n\")\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.strip().lower() in ['exit', 'quit']:\n",
    "        print(\"Exiting chat. Goodbye!\")\n",
    "        break\n",
    "    chat_with_ollama(user_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82a44f7",
   "metadata": {},
   "source": [
    "### For Enhancements:\n",
    "* Use LangChain to manage memory\n",
    "\n",
    "* Add voice input/output\n",
    "\n",
    "* Connect to Streamlit for a web UI\n",
    "\n",
    "* Save/load conversation history in a JSON file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470e490a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
