{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99420692",
   "metadata": {},
   "source": [
    "# Decoder Causal Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70dbe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import multi30k, Multi30k\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from typing import Iterable, List\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.datasets import IMDB,PennTreebank\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import time\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ad92c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "train_iter, val_iter = IMDB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8671d7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_itr=iter(train_iter)\n",
    "# retrieving the third first record\n",
    "next(data_itr)\n",
    "next(data_itr)\n",
    "next(data_itr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991d3e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c2964e",
   "metadata": {},
   "source": [
    "\n",
    "## Preprocessing data\n",
    "The provided code is used for preprocessing text data, particularly for NLP tasks, with a focus on tokenization and vocabulary building.\n",
    "\n",
    "- **Special Symbols and Indices**: Initializes special tokens (`<unk>`, `<pad>`, and an empty string for EOS) with their corresponding indices (`0`, `1`, and `2`). These tokens are used for unknown words, padding, and end of sentence respectively.\n",
    "    - `UNK_IDX`: Index for unknown words.\n",
    "    - `PAD_IDX`: Index used for padding shorter sentences in a batch to ensure uniform length.\n",
    "    - `EOS_IDX`: Index representing the end of a sentence (though not explicitly used here as the EOS symbol is set to an empty string).\n",
    "\n",
    "- **`yield_tokens` Function**: A generator function that iterates through a dataset (`data_iter`), tokenizing each data sample using a `tokenizer` function, and yields one tokenized sample at a time.\n",
    "\n",
    "- **Vocabulary building**: Constructs a vocabulary from the tokenized dataset. The `build_vocab_from_iterator` function processes tokens generated by `yield_tokens`, includes special tokens (`special_symbols`) at the beginning of the vocabulary, and sets a minimum frequency (`min_freq=1`) for tokens to be included.\n",
    "\n",
    "- **Default index for unknown tokens**: Sets a default index for tokens not found in the vocabulary (`UNK_IDX`), ensuring that out-of-vocabulary words are handled as unknown tokens.\n",
    "\n",
    "- **`text_to_index` function**: Converts a given text into a sequence of indices based on the built vocabulary. This function is essential for transforming raw text into a numerical format that can be processed by machine learning models.\n",
    "\n",
    "- **`index_to_en` function**: Transforms a sequence of indices back into a readable string. It's useful for interpreting the outputs of models and converting numerical predictions back into text.\n",
    "\n",
    "- **Check functionality**: Demonstrates the use of `index_to_en` by converting a tensor of indices `[0,1,2]` back into their corresponding special symbols. This helps verify that the vocabulary and index conversion functions are working as expected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7da008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, EOS_IDX = 0, 1, 2\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<|endoftext|>' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f80c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ad1a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter):\n",
    "\n",
    "    for _,data_sample in data_iter:\n",
    "        yield  tokenizer(data_sample)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=special_symbols, special_first=True)\n",
    "vocab.set_default_index(UNK_IDX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526a83fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_index=lambda text: [vocab(token) for token in tokenizer(text)]\n",
    "index_to_en = lambda seq_en: \" \".join([vocab.get_itos()[index] for index in seq_en])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb872dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check\n",
    "index_to_en(torch.tensor([0,1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e90f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample(block_size, text):\n",
    "    # Determine the length of the input text\n",
    "    sample_leg = len(text)\n",
    "    # Calculate the stopping point for randomly selecting a sample\n",
    "    # This ensures the selected sample doesn't exceed the text length\n",
    "    random_sample_stop = sample_leg - block_size\n",
    "\n",
    "\n",
    "    # Check if a random sample can be taken (if the text is longer than block_size)\n",
    "    if random_sample_stop >= 1:\n",
    "        # Randomly select a starting point for the sample\n",
    "        random_start = torch.randint(low=0, high=random_sample_stop, size=(1,)).item()\n",
    "        # Define the endpoint of the sample\n",
    "        stop = random_start + block_size\n",
    "\n",
    "        # Create the input and target sequences\n",
    "        src_sequence = text[random_start:stop]\n",
    "        tgt_sequence= text[random_start + 1:stop + 1]\n",
    "\n",
    "    # Handle the case where the text length is exactly equal or less the block size\n",
    "    elif random_sample_stop <= 0:\n",
    "        # Start from the beginning and use the entire text\n",
    "        random_start = 0\n",
    "        stop = sample_leg\n",
    "        src_sequence= text[random_start:stop]\n",
    "        tgt_sequence = text[random_start + 1:stop]\n",
    "        # Append an empty string to maintain sequence alignment\n",
    "        tgt_sequence.append( '<|endoftext|>')\n",
    "\n",
    "    return src_sequence, tgt_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe1c577",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=1\n",
    "\n",
    "batch_of_tokens=[]\n",
    "\n",
    "for i in range(BATCH_SIZE):\n",
    "  _,text =next(iter(train_iter))\n",
    "  batch_of_tokens.append(tokenizer(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e2133e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=batch_of_tokens[0][0:100]\n",
    "text[0:100]\n",
    "batch_of_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a943134b",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size=10\n",
    "src_sequences, tgt_sequence=get_sample( block_size, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4ce29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"src: \",src_sequences)\n",
    "print(\"tgt: \",tgt_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5592e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty lists to store source and target sequences\n",
    "src_batch, tgt_batch = [], []\n",
    "\n",
    "# Define the batch size\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "# Loop to create batches of source and target sequences\n",
    "for i in range(BATCH_SIZE):\n",
    "    # Retrieve the next data point from the training iterator\n",
    "    _,text = next(iter(train_iter))\n",
    "\n",
    "    # Generate source and target sequences using the get_sample function\n",
    "    src_sequence_text, tgt_sequence_text = get_sample(block_size, tokenizer(text))\n",
    "\n",
    "    # Convert source and target sequences to tokenized vocabulary indices\n",
    "    src_sequence_indices = vocab(src_sequence_text)\n",
    "    tgt_sequence_indices = vocab(tgt_sequence_text)\n",
    "\n",
    "    # Convert the sequences to PyTorch tensors with dtype int64\n",
    "    src_sequence = torch.tensor(src_sequence_indices, dtype=torch.int64)\n",
    "    tgt_sequence = torch.tensor(tgt_sequence_indices, dtype=torch.int64)\n",
    "\n",
    "    # Append the source and target sequences to their respective batches\n",
    "    src_batch.append(src_sequence)\n",
    "    tgt_batch.append(tgt_sequence)\n",
    "\n",
    "    # Print the output for every 2nd sample (adjust as needed)\n",
    "    print(f\"Sample {i}:\")\n",
    "    print(\"Source Sequence (Text):\", src_sequence_text)\n",
    "    print(\"Source Sequence (Indices):\", src_sequence_indices)\n",
    "    print(\"Source Sequence (Shape):\", src_sequence.shape)\n",
    "    print(\"Target Sequence (Text):\", tgt_sequence_text)\n",
    "    print(\"Target Sequence (Indices):\", tgt_sequence_indices)\n",
    "    print(\"Target Sequence (Shape):\", tgt_sequence.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c867171",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
