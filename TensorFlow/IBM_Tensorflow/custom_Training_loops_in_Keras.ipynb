{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "490061d4",
   "metadata": {},
   "source": [
    "implement a basic custom training loop in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbd2a2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "915ffd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress all Python warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set TensorFlow log level to suppress warnings and info messages\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Step 1: Set Up the Environment\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data() \n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0 \n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2764874",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed026c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb0c72d",
   "metadata": {},
   "source": [
    "### Define loss function and optimizer\n",
    "\n",
    "- Use Sparse Categorical Crossentropy for the loss function. \n",
    "- Use the Adam optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4db7c092",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) \n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b51c85d",
   "metadata": {},
   "source": [
    "### Implement custom training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2bc18ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 1\n",
      "Epoch 1 Step 0: Loss = 2.3339295387268066\n",
      "Epoch 1 Step 200: Loss = 0.4200586676597595\n",
      "Epoch 1 Step 400: Loss = 0.18353486061096191\n",
      "Epoch 1 Step 600: Loss = 0.16809335350990295\n",
      "Epoch 1 Step 800: Loss = 0.15435132384300232\n",
      "Epoch 1 Step 1000: Loss = 0.5205732583999634\n",
      "Epoch 1 Step 1200: Loss = 0.18945136666297913\n",
      "Epoch 1 Step 1400: Loss = 0.24824224412441254\n",
      "Epoch 1 Step 1600: Loss = 0.17942407727241516\n",
      "Epoch 1 Step 1800: Loss = 0.1575031876564026\n",
      "Start of epoch 2\n",
      "Epoch 2 Step 0: Loss = 0.05862722545862198\n",
      "Epoch 2 Step 200: Loss = 0.21767151355743408\n",
      "Epoch 2 Step 400: Loss = 0.12814423441886902\n",
      "Epoch 2 Step 600: Loss = 0.04776795953512192\n",
      "Epoch 2 Step 800: Loss = 0.0879962220788002\n",
      "Epoch 2 Step 1000: Loss = 0.346660315990448\n",
      "Epoch 2 Step 1200: Loss = 0.08232636004686356\n",
      "Epoch 2 Step 1400: Loss = 0.15419481694698334\n",
      "Epoch 2 Step 1600: Loss = 0.1385653018951416\n",
      "Epoch 2 Step 1800: Loss = 0.10450499504804611\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "# train_dataset = train_dataset.repeat(epochs)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "for epoch in range(epochs):\n",
    "    print(f'Start of epoch {epoch + 1}')\n",
    "\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch_train, training=True)  # Forward pass\n",
    "            loss_value = loss_fn(y_batch_train, logits)  # Compute loss\n",
    "\n",
    "        # Compute gradients and update weights\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        # Logging the loss every 200 steps\n",
    "        if step % 200 == 0:\n",
    "            print(f'Epoch {epoch + 1} Step {step}: Loss = {loss_value.numpy()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359be8a2",
   "metadata": {},
   "source": [
    "### Adding accuracy metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "668a52e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Flatten \n",
    "\n",
    "# Step 1: Set Up the Environment\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the pixel values to be between 0 and 1\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0 \n",
    "\n",
    "# Create a batched dataset for training\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb4bfd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([ \n",
    "    Flatten(input_shape=(28, 28)),  # Flatten the input to a 1D vector\n",
    "    Dense(128, activation='relu'),  # First hidden layer with 128 neurons and ReLU activation\n",
    "    Dense(10)  # Output layer with 10 neurons for the 10 classes (digits 0-9)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a486d273",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Loss function for multi-class classification\n",
    "optimizer = tf.keras.optimizers.Adam()  # Adam optimizer for efficient training\n",
    "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy()  # Metric to track accuracy during training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16aa856",
   "metadata": {},
   "source": [
    "### Implement loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12267921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 1\n",
      "Epoch 1 Step 0: Loss = 2.424161434173584 Accuracy = 0.09375\n",
      "Epoch 1 Step 200: Loss = 0.3653165400028229 Accuracy = 0.8387748599052429\n",
      "Epoch 1 Step 400: Loss = 0.16105180978775024 Accuracy = 0.8706359267234802\n",
      "Epoch 1 Step 600: Loss = 0.18409746885299683 Accuracy = 0.8850353360176086\n",
      "Epoch 1 Step 800: Loss = 0.14532171189785004 Accuracy = 0.8971598148345947\n",
      "Epoch 1 Step 1000: Loss = 0.44887739419937134 Accuracy = 0.9036276340484619\n",
      "Epoch 1 Step 1200: Loss = 0.17933422327041626 Accuracy = 0.9098407626152039\n",
      "Epoch 1 Step 1400: Loss = 0.2197543829679489 Accuracy = 0.9147260785102844\n",
      "Epoch 1 Step 1600: Loss = 0.21381160616874695 Accuracy = 0.9177271723747253\n",
      "Epoch 1 Step 1800: Loss = 0.17532978951931 Accuracy = 0.9216927886009216\n",
      "Start of epoch 2\n",
      "Epoch 2 Step 0: Loss = 0.07789589464664459 Accuracy = 1.0\n",
      "Epoch 2 Step 200: Loss = 0.14208102226257324 Accuracy = 0.9597325921058655\n",
      "Epoch 2 Step 400: Loss = 0.12307007610797882 Accuracy = 0.9571384191513062\n",
      "Epoch 2 Step 600: Loss = 0.05692485719919205 Accuracy = 0.9590786099433899\n",
      "Epoch 2 Step 800: Loss = 0.06584063172340393 Accuracy = 0.9604010581970215\n",
      "Epoch 2 Step 1000: Loss = 0.278097003698349 Accuracy = 0.9606019258499146\n",
      "Epoch 2 Step 1200: Loss = 0.10797001421451569 Accuracy = 0.9615424871444702\n",
      "Epoch 2 Step 1400: Loss = 0.16460268199443817 Accuracy = 0.962549090385437\n",
      "Epoch 2 Step 1600: Loss = 0.1849026083946228 Accuracy = 0.9624063372612\n",
      "Epoch 2 Step 1800: Loss = 0.10795281827449799 Accuracy = 0.9633883833885193\n",
      "Start of epoch 3\n",
      "Epoch 3 Step 0: Loss = 0.030420074239373207 Accuracy = 1.0\n",
      "Epoch 3 Step 200: Loss = 0.10701996088027954 Accuracy = 0.9771454930305481\n",
      "Epoch 3 Step 400: Loss = 0.10221768915653229 Accuracy = 0.9747506380081177\n",
      "Epoch 3 Step 600: Loss = 0.041363056749105453 Accuracy = 0.9751976132392883\n",
      "Epoch 3 Step 800: Loss = 0.05375022068619728 Accuracy = 0.9747191071510315\n",
      "Epoch 3 Step 1000: Loss = 0.13329052925109863 Accuracy = 0.9747751951217651\n",
      "Epoch 3 Step 1200: Loss = 0.06733254343271255 Accuracy = 0.9751769304275513\n",
      "Epoch 3 Step 1400: Loss = 0.08193515986204147 Accuracy = 0.9754639267921448\n",
      "Epoch 3 Step 1600: Loss = 0.08216576278209686 Accuracy = 0.9752888679504395\n",
      "Epoch 3 Step 1800: Loss = 0.05636395141482353 Accuracy = 0.9757946729660034\n",
      "Start of epoch 4\n",
      "Epoch 4 Step 0: Loss = 0.020692437887191772 Accuracy = 1.0\n",
      "Epoch 4 Step 200: Loss = 0.07037060707807541 Accuracy = 0.9833644032478333\n",
      "Epoch 4 Step 400: Loss = 0.0918084904551506 Accuracy = 0.9811409115791321\n",
      "Epoch 4 Step 600: Loss = 0.02926451712846756 Accuracy = 0.9824771285057068\n",
      "Epoch 4 Step 800: Loss = 0.04819932579994202 Accuracy = 0.9824438095092773\n",
      "Epoch 4 Step 1000: Loss = 0.09902288019657135 Accuracy = 0.9821740984916687\n",
      "Epoch 4 Step 1200: Loss = 0.03162672370672226 Accuracy = 0.9820722341537476\n",
      "Epoch 4 Step 1400: Loss = 0.04885308817028999 Accuracy = 0.9824009537696838\n",
      "Epoch 4 Step 1600: Loss = 0.03742845728993416 Accuracy = 0.9822962284088135\n",
      "Epoch 4 Step 1800: Loss = 0.036920610815286636 Accuracy = 0.9826658964157104\n",
      "Start of epoch 5\n",
      "Epoch 5 Step 0: Loss = 0.036573901772499084 Accuracy = 0.96875\n",
      "Epoch 5 Step 200: Loss = 0.03519917652010918 Accuracy = 0.9886505007743835\n",
      "Epoch 5 Step 400: Loss = 0.0784674882888794 Accuracy = 0.9872973561286926\n",
      "Epoch 5 Step 600: Loss = 0.03907008469104767 Accuracy = 0.9875208139419556\n",
      "Epoch 5 Step 800: Loss = 0.027628093957901 Accuracy = 0.9875936508178711\n",
      "Epoch 5 Step 1000: Loss = 0.08802398294210434 Accuracy = 0.9873875975608826\n",
      "Epoch 5 Step 1200: Loss = 0.024501509964466095 Accuracy = 0.9871981739997864\n",
      "Epoch 5 Step 1400: Loss = 0.021899063140153885 Accuracy = 0.9874197244644165\n",
      "Epoch 5 Step 1600: Loss = 0.02601497620344162 Accuracy = 0.9872150421142578\n",
      "Epoch 5 Step 1800: Loss = 0.026158761233091354 Accuracy = 0.9875416159629822\n"
     ]
    }
   ],
   "source": [
    "epochs = 5  # Number of epochs for training\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Start of epoch {epoch + 1}')\n",
    "    \n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass: Compute predictions\n",
    "            logits = model(x_batch_train, training=True)\n",
    "            # Compute loss\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "        \n",
    "        # Compute gradients\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        # Apply gradients to update model weights\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        \n",
    "        # Update the accuracy metric\n",
    "        accuracy_metric.update_state(y_batch_train, logits)\n",
    "\n",
    "        # Log the loss and accuracy every 200 steps\n",
    "        if step % 200 == 0:\n",
    "            print(f'Epoch {epoch + 1} Step {step}: Loss = {loss_value.numpy()} Accuracy = {accuracy_metric.result().numpy()}')\n",
    "    \n",
    "    # Reset the metric at the end of each epoch\n",
    "    accuracy_metric.reset_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfacc8e",
   "metadata": {},
   "source": [
    "### Custom Callback for Advanced logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6a49b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Flatten \n",
    "\n",
    "# Step 1: Set Up the Environment\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the pixel values to be between 0 and 1\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0 \n",
    "\n",
    "# Create a batched dataset for training\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e486b127",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),  # Flatten the input to a 1D vector\n",
    "    Dense(128, activation='relu'),  # First hidden layer with 128 neurons and ReLU activation\n",
    "    Dense(10)  # Output layer with 10 neurons for the 10 classes (digits 0-9)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02c03053",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Loss function for multi-class classification\n",
    "optimizer = tf.keras.optimizers.Adam()  # Adam optimizer for efficient training\n",
    "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy()  # Metric to track accuracy during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a44023e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "# Step 4: Implement the Custom Callback \n",
    "class CustomCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        print(f'End of epoch {epoch + 1}, loss: {logs.get(\"loss\")}, accuracy: {logs.get(\"accuracy\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02e099e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 1\n",
      "Epoch 1 Step 0: Loss = 2.349398612976074 Accuracy = 0.03125\n",
      "Epoch 1 Step 200: Loss = 0.41247522830963135 Accuracy = 0.8373756408691406\n",
      "Epoch 1 Step 400: Loss = 0.19316187500953674 Accuracy = 0.8695448637008667\n",
      "Epoch 1 Step 600: Loss = 0.1684657633304596 Accuracy = 0.8850873708724976\n",
      "Epoch 1 Step 800: Loss = 0.15048527717590332 Accuracy = 0.896691620349884\n",
      "Epoch 1 Step 1000: Loss = 0.403544545173645 Accuracy = 0.9041271209716797\n",
      "Epoch 1 Step 1200: Loss = 0.23036609590053558 Accuracy = 0.9101269841194153\n",
      "Epoch 1 Step 1400: Loss = 0.24016696214675903 Accuracy = 0.9151945114135742\n",
      "Epoch 1 Step 1600: Loss = 0.18693406879901886 Accuracy = 0.9182737469673157\n",
      "Epoch 1 Step 1800: Loss = 0.20807109773159027 Accuracy = 0.9219183921813965\n",
      "End of epoch 1, loss: 0.0444197840988636, accuracy: 0.9239333271980286\n",
      "Start of epoch 2\n",
      "Epoch 2 Step 0: Loss = 0.09903012961149216 Accuracy = 0.96875\n",
      "Epoch 2 Step 200: Loss = 0.20589420199394226 Accuracy = 0.9622201323509216\n",
      "Epoch 2 Step 400: Loss = 0.10010591149330139 Accuracy = 0.957761824131012\n",
      "Epoch 2 Step 600: Loss = 0.06097021698951721 Accuracy = 0.9594426155090332\n",
      "Epoch 2 Step 800: Loss = 0.10613700747489929 Accuracy = 0.9605961441993713\n",
      "Epoch 2 Step 1000: Loss = 0.21753424406051636 Accuracy = 0.9610389471054077\n",
      "Epoch 2 Step 1200: Loss = 0.17918823659420013 Accuracy = 0.961724579334259\n",
      "Epoch 2 Step 1400: Loss = 0.09845925867557526 Accuracy = 0.962549090385437\n",
      "Epoch 2 Step 1600: Loss = 0.14224129915237427 Accuracy = 0.9628552198410034\n",
      "Epoch 2 Step 1800: Loss = 0.13416776061058044 Accuracy = 0.963718056678772\n",
      "End of epoch 2, loss: 0.0574057474732399, accuracy: 0.9643666744232178\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "custom_callback = CustomCallback()  # Initialize the custom callback\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Start of epoch {epoch + 1}')\n",
    "    \n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass: Compute predictions\n",
    "            logits = model(x_batch_train, training=True)\n",
    "            # Compute loss\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "        \n",
    "        # Compute gradients\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        # Apply gradients to update model weights\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        \n",
    "        # Update the accuracy metric\n",
    "        accuracy_metric.update_state(y_batch_train, logits)\n",
    "\n",
    "        # Log the loss and accuracy every 200 steps\n",
    "        if step % 200 == 0:\n",
    "            print(f'Epoch {epoch + 1} Step {step}: Loss = {loss_value.numpy()} Accuracy = {accuracy_metric.result().numpy()}')\n",
    "    \n",
    "    # Call the custom callback at the end of each epoch\n",
    "    custom_callback.on_epoch_end(epoch, logs={'loss': loss_value.numpy(), 'accuracy': accuracy_metric.result().numpy()})\n",
    "    \n",
    "    # Reset the metric at the end of each epoch\n",
    "    accuracy_metric.reset_state()  # Use reset_state() instead of reset_states()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0261a979",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "# Define the input layer\n",
    "input_layer = Input(shape=(28, 28))  # Input layer with shape (28, 28)\n",
    "\n",
    "# Define hidden layers\n",
    "hidden_layer1 = Dense(64, activation='relu')(input_layer)  # First hidden layer with 64 neurons and ReLU activation\n",
    "hidden_layer2 = Dense(64, activation='relu')(hidden_layer1)  # Second hidden layer with 64 neurons and ReLU activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa43f0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer = Dense(1, activation='sigmoid')(hidden_layer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2d9bea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=input_layer, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b43ce9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "328ce3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 930us/step - accuracy: 0.5006 - loss: 0.6997 \n",
      "Epoch 2/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 941us/step - accuracy: 0.4893 - loss: 0.6932\n",
      "Epoch 3/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 975us/step - accuracy: 0.5291 - loss: 0.6897\n",
      "Epoch 4/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 952us/step - accuracy: 0.5220 - loss: 0.6908\n",
      "Epoch 5/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 979us/step - accuracy: 0.5601 - loss: 0.6851\n",
      "Epoch 6/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 906us/step - accuracy: 0.5244 - loss: 0.6875\n",
      "Epoch 7/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 942us/step - accuracy: 0.5681 - loss: 0.6856\n",
      "Epoch 8/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5725 - loss: 0.6851 \n",
      "Epoch 9/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 963us/step - accuracy: 0.5494 - loss: 0.6833\n",
      "Epoch 10/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 936us/step - accuracy: 0.5907 - loss: 0.6788\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x331c6f980>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Redefine the Model for 20 features\n",
    "model = Sequential([\n",
    "    Input(shape=(20,)),  # Adjust input shape to (20,)\n",
    "    Dense(128, activation='relu'),  # Hidden layer with 128 neurons and ReLU activation\n",
    "    Dense(1, activation='sigmoid')  # Output layer for binary classification with sigmoid activation\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 2: Generate Example Data\n",
    "X_train = np.random.rand(1000, 20)  # 1000 samples, 20 features each\n",
    "y_train = np.random.randint(2, size=(1000, 1))  # 1000 binary labels (0 or 1)\n",
    "\n",
    "# Step 3: Train the Model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aea16220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5355 - loss: 0.6937 \n",
      "Test loss: 0.6935969591140747\n",
      "Test accuracy: 0.5199999809265137\n"
     ]
    }
   ],
   "source": [
    "# Example test data (in practice, use real dataset)\n",
    "X_test = np.random.rand(400, 20)  # 200 samples, 20 features each\n",
    "y_test = np.random.randint(2, size=(400, 1))  # 200 binary labels (0 or 1)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Print test loss and accuracy\n",
    "print(f'Test loss: {loss}')\n",
    "print(f'Test accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795b6411",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
