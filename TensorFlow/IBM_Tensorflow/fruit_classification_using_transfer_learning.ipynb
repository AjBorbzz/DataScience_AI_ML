{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff3ddecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"keras.src.trainers.data_adapters.py_dataset_adapter\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"keras.src.trainers.epoch_iterator\")\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress all warnings and info messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e351f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading the dataset...\n",
      "Download completed.\n",
      "Extracting the dataset in chunks...\n",
      "Extracted 2000 of 12481 files...\n",
      "Extracted 4000 of 12481 files...\n",
      "Extracted 6000 of 12481 files...\n",
      "Extracted 8000 of 12481 files...\n",
      "Extracted 10000 of 12481 files...\n",
      "Extracted 12000 of 12481 files...\n",
      "Extracted 12481 of 12481 files...\n",
      "Dataset successfully extracted to 'fruits-360-original-size'.\n",
      "Cleaned up zip file: fruits-360-original-size.zip\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import zipfile\n",
    "\n",
    "# Define dataset URL and paths\n",
    "url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/4yIRGlIpNfKEGJYMhZV52g/fruits-360-original-size.zip\"\n",
    "\n",
    "local_zip = \"fruits-360-original-size.zip\"\n",
    "extract_dir = \"fruits-360-original-size\"\n",
    "\n",
    "def download_dataset(url, output_file):\n",
    "    \"\"\"Download the dataset using wget in quiet mode.\"\"\"\n",
    "    print(\"Downloading the dataset...\")\n",
    "    subprocess.run([\"wget\", \"-q\", \"-O\", output_file, url], check=True)  # Add `-q` for quiet mode\n",
    "    print(\"Download completed.\")\n",
    "\n",
    "def extract_zip_in_chunks(zip_file, extract_to, batch_size=2000):\n",
    "    \"\"\"\n",
    "    Extract a large zip file in chunks to avoid memory bottlenecks.\n",
    "    Processes a specified number of files (batch_size) at a time.\n",
    "    \"\"\"\n",
    "    print(\"Extracting the dataset in chunks...\")\n",
    "    os.makedirs(extract_to, exist_ok=True)  # Ensure the extraction directory exists\n",
    "    \n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        files = zip_ref.namelist()  # List all files in the archive\n",
    "        total_files = len(files)\n",
    "        \n",
    "        for i in range(0, total_files, batch_size):\n",
    "            batch = files[i:i+batch_size]\n",
    "            for file in batch:\n",
    "                zip_ref.extract(file, extract_to)  # Extract each file in the batch\n",
    "            print(f\"Extracted {min(i+batch_size, total_files)} of {total_files} files...\")\n",
    "    \n",
    "    print(f\"Dataset successfully extracted to '{extract_to}'.\")\n",
    "\n",
    "# Main script execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Download the dataset if not already downloaded\n",
    "    if not os.path.exists(local_zip):\n",
    "        download_dataset(url, local_zip)\n",
    "    else:\n",
    "        print(\"Dataset already downloaded.\")\n",
    "    \n",
    "    # Extract the dataset if not already extracted\n",
    "    if not os.path.exists(extract_dir):\n",
    "        extract_zip_in_chunks(local_zip, extract_dir)\n",
    "    else:\n",
    "        print(\"Dataset already extracted.\")\n",
    "    \n",
    "    # Optional cleanup of the zip file\n",
    "    if os.path.exists(local_zip):\n",
    "        os.remove(local_zip)\n",
    "        print(f\"Cleaned up zip file: {local_zip}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3e076b",
   "metadata": {},
   "source": [
    "### Import necessary libraries and set dataset paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab9f0c4",
   "metadata": {},
   "source": [
    "- `ImageDataGenerator:` For loading images and applying data augmentation.\n",
    "- `VGG16:` Pre-trained model used for transfer learning.\n",
    "- `Sequential:` For building a sequential model.\n",
    "- `Dense, Flatten, Dropout, BatchNormalization:` Layers to customize the model architecture.\n",
    "- `ReduceLROnPlateau, EarlyStopping:` Callbacks for optimizing training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dcabe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "# Set dataset paths\n",
    "train_dir = 'fruits-360-original-size/fruits-360-original-size/Training'\n",
    "val_dir = 'fruits-360-original-size/fruits-360-original-size/Validation'\n",
    "test_dir = 'fruits-360-original-size/fruits-360-original-size/Test'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7104befe",
   "metadata": {},
   "source": [
    "### Set up data generators for training, validation, and testing with augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1927200d",
   "metadata": {},
   "source": [
    "- `train_datagen:` Applies rescaling and augmentation (e.g., rotation, zoom) to make the model more robust.\n",
    "- `val_datagen and test_datagen:` Only rescale images for validation/testing.\n",
    "- `flow_from_directory:` Loads images from specified folders into batches for training/validation/testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f89fa6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6231 images belonging to 24 classes.\n",
      "Found 3114 images belonging to 24 classes.\n",
      "Found 3110 images belonging to 24 classes.\n"
     ]
    }
   ],
   "source": [
    "# Image data generators\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255.0,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "# Load images from directories\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(64, 64),\n",
    "    batch_size=16,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(64, 64),\n",
    "    batch_size=16,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(64, 64),\n",
    "    batch_size=16,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eacd378",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
