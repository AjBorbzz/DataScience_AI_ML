{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18e82342",
   "metadata": {},
   "source": [
    "## Project Introduction: Document Classification with Machine Learning\n",
    "\n",
    "In this personal project, I explore how machine learning can be used to automatically classify news articles into relevant categories. This kind of document classification is a key component of intelligent content search engines, helping users find information quickly and accurately.\n",
    "\n",
    "To implement this, I’ll use **PyTorch** and the **torchtext** library to preprocess and prepare raw text data for model training. The pipeline includes converting text into tensors, organizing it into batches, and building a classifier capable of predicting the topic of a given article.\n",
    "\n",
    "This project demonstrates the practical application of natural language processing (NLP) and deep learning in organizing large volumes of unstructured text—a valuable capability for search, recommendation, and information retrieval systems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31633dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b7862b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "class NewsDataset(Dataset):\n",
    "    \"\"\"Custom Dataset class for news articles\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, labels, vocab, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenize and convert to indices\n",
    "        tokens = self.tokenizer(text.lower())\n",
    "        indices = [self.vocab.get(token, self.vocab['<UNK>']) for token in tokens]\n",
    "        \n",
    "        # Pad or truncate to max_length\n",
    "        if len(indices) > self.max_length:\n",
    "            indices = indices[:self.max_length]\n",
    "        else:\n",
    "            indices.extend([self.vocab['<PAD>']] * (self.max_length - len(indices)))\n",
    "            \n",
    "        return torch.tensor(indices, dtype=torch.long), torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e19843",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    \"\"\"Neural network model for text classification\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, n_layers=2, dropout=0.3):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, n_layers, \n",
    "                           dropout=dropout if n_layers > 1 else 0, \n",
    "                           batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # *2 for bidirectional\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_length)\n",
    "        embedded = self.embedding(x)  # (batch_size, seq_length, embed_dim)\n",
    "        \n",
    "        # LSTM layer\n",
    "        lstm_out, (hidden, _) = self.lstm(embedded)\n",
    "        \n",
    "        # Use the last hidden state (forward and backward)\n",
    "        # hidden shape: (n_layers*2, batch_size, hidden_dim)\n",
    "        forward_hidden = hidden[-2]  # Last layer forward\n",
    "        backward_hidden = hidden[-1]  # Last layer backward\n",
    "        \n",
    "        # Concatenate forward and backward hidden states\n",
    "        final_hidden = torch.cat((forward_hidden, backward_hidden), dim=1)\n",
    "        \n",
    "        # Apply dropout and final linear layer\n",
    "        output = self.dropout(final_hidden)\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1243e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsClassifier:\n",
    "    \"\"\"Main class for news article classification\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim=128, hidden_dim=256, max_length=512):\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = get_tokenizer('basic_english')\n",
    "        self.vocab = None\n",
    "        self.label_to_idx = None\n",
    "        self.idx_to_label = None\n",
    "        self.model = None\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "    def build_vocab(self, texts, min_freq=2):\n",
    "        \"\"\"Build vocabulary from training texts\"\"\"\n",
    "        print(\"Building vocabulary...\")\n",
    "        \n",
    "        # Tokenize all texts and count word frequencies\n",
    "        counter = Counter()\n",
    "        for text in texts:\n",
    "            tokens = self.tokenizer(str(text).lower())\n",
    "            counter.update(tokens)\n",
    "        \n",
    "        # Create vocabulary with special tokens\n",
    "        vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "        for word, freq in counter.items():\n",
    "            if freq >= min_freq:\n",
    "                vocab[word] = len(vocab)\n",
    "                \n",
    "        print(f\"Vocabulary size: {len(vocab)}\")\n",
    "        self.vocab = vocab\n",
    "        return vocab\n",
    "\n",
    "\n",
    "    def prepare_labels(self, labels):\n",
    "        \"\"\"Convert labels to numerical indices\"\"\"\n",
    "        unique_labels = sorted(list(set(labels)))\n",
    "        self.label_to_idx = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "        self.idx_to_label = {idx: label for label, idx in self.label_to_idx.items()}\n",
    "        \n",
    "        numerical_labels = [self.label_to_idx[label] for label in labels]\n",
    "        return numerical_labels\n",
    "    \n",
    "    def create_data_loaders(self, train_texts, train_labels, test_texts, test_labels, batch_size=32):\n",
    "        \"\"\"Create PyTorch data loaders\"\"\"\n",
    "        train_dataset = NewsDataset(train_texts, train_labels, self.vocab, \n",
    "                                  self.tokenizer, self.max_length)\n",
    "        test_dataset = NewsDataset(test_texts, test_labels, self.vocab, \n",
    "                                 self.tokenizer, self.max_length)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        return train_loader, test_loader\n",
    "    \n",
    "    def build_model(self, num_classes):\n",
    "        \"\"\"Build the neural network model\"\"\"\n",
    "        vocab_size = len(self.vocab)\n",
    "        self.model = TextClassifier(\n",
    "            vocab_size=vocab_size,\n",
    "            embed_dim=self.embed_dim,\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            output_dim=num_classes\n",
    "        ).to(self.device)\n",
    "        \n",
    "        print(f\"Model built with {sum(p.numel() for p in self.model.parameters())} parameters\")\n",
    "        return self.model\n",
    "\n",
    "\n",
    "    def train(self, train_loader, test_loader, num_epochs=10, learning_rate=0.001):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        print(f\"Training on {self.device}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            # Training phase\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "            correct_predictions = 0\n",
    "            total_samples = 0\n",
    "            \n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                output = self.model(data)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                pred = output.argmax(dim=1)\n",
    "                correct_predictions += (pred == target).sum().item()\n",
    "                total_samples += target.size(0)\n",
    "            \n",
    "            # Calculate training accuracy\n",
    "            train_accuracy = correct_predictions / total_samples\n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            \n",
    "            # Validation phase\n",
    "            test_accuracy = self.evaluate(test_loader)\n",
    "            \n",
    "            print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "            print(f'  Train Loss: {avg_loss:.4f} | Train Acc: {train_accuracy:.4f}')\n",
    "            print(f'  Test Acc: {test_accuracy:.4f}')\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "    def evaluate(self, test_loader):\n",
    "        \"\"\"Evaluate the model\"\"\"\n",
    "        self.model.eval()\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                output = self.model(data)\n",
    "                pred = output.argmax(dim=1)\n",
    "                correct_predictions += (pred == target).sum().item()\n",
    "                total_samples += target.size(0)\n",
    "        \n",
    "        accuracy = correct_predictions / total_samples\n",
    "        return accuracy\n",
    "\n",
    "    def predict(self, texts):\n",
    "        \"\"\"Make predictions on new texts\"\"\"\n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for text in texts:\n",
    "                # Tokenize and convert to indices\n",
    "                tokens = self.tokenizer(str(text).lower())\n",
    "                indices = [self.vocab.get(token, self.vocab['<UNK>']) for token in tokens]\n",
    "                \n",
    "                # Pad or truncate\n",
    "                if len(indices) > self.max_length:\n",
    "                    indices = indices[:self.max_length]\n",
    "                else:\n",
    "                    indices.extend([self.vocab['<PAD>']] * (self.max_length - len(indices)))\n",
    "                \n",
    "                # Convert to tensor and add batch dimension\n",
    "                input_tensor = torch.tensor([indices], dtype=torch.long).to(self.device)\n",
    "                \n",
    "                # Make prediction\n",
    "                output = self.model(input_tensor)\n",
    "                pred_idx = output.argmax(dim=1).item()\n",
    "                pred_label = self.idx_to_label[pred_idx]\n",
    "                predictions.append(pred_label)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "def save_model(self, filepath):\n",
    "        \"\"\"Save the trained model and vocabulary\"\"\"\n",
    "        checkpoint = {\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'vocab': self.vocab,\n",
    "            'label_to_idx': self.label_to_idx,\n",
    "            'idx_to_label': self.idx_to_label,\n",
    "            'embed_dim': self.embed_dim,\n",
    "            'hidden_dim': self.hidden_dim,\n",
    "            'max_length': self.max_length\n",
    "        }\n",
    "        torch.save(checkpoint, filepath)\n",
    "        print(f\"Model saved to {filepath}\")\n",
    "\n",
    "def load_model(self, filepath, num_classes):\n",
    "        \"\"\"Load a trained model\"\"\"\n",
    "        checkpoint = torch.load(filepath, map_location=self.device)\n",
    "        \n",
    "        self.vocab = checkpoint['vocab']\n",
    "        self.label_to_idx = checkpoint['label_to_idx']\n",
    "        self.idx_to_label = checkpoint['idx_to_label']\n",
    "        self.embed_dim = checkpoint['embed_dim']\n",
    "        self.hidden_dim = checkpoint['hidden_dim']\n",
    "        self.max_length = checkpoint['max_length']\n",
    "        \n",
    "        self.build_model(num_classes)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"Model loaded from {filepath}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e099ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample usage\n",
    "def create_sample_data():\n",
    "    \"\"\"Create sample news data for demonstration\"\"\"\n",
    "    sample_data = [\n",
    "        # Technology articles\n",
    "        (\"Apple releases new iPhone with advanced AI capabilities and improved camera system\", \"Technology\"),\n",
    "        (\"Google announces breakthrough in quantum computing research\", \"Technology\"),\n",
    "        (\"Microsoft launches new cloud computing platform for enterprises\", \"Technology\"),\n",
    "        (\"Tesla unveils latest electric vehicle model with autonomous driving features\", \"Technology\"),\n",
    "        (\"Meta develops new virtual reality headset for gaming and productivity\", \"Technology\"),\n",
    "        \n",
    "        # Sports articles\n",
    "        (\"Basketball championship finals draw record-breaking television audience\", \"Sports\"),\n",
    "        (\"Olympic swimming records broken in international competition\", \"Sports\"),\n",
    "        (\"Football team wins championship after dramatic overtime victory\", \"Sports\"),\n",
    "        (\"Tennis tournament features intense matches between top-ranked players\", \"Sports\"),\n",
    "        (\"Soccer world cup preparations underway in host country\", \"Sports\"),\n",
    "        \n",
    "        # Politics articles\n",
    "        (\"Congress passes new legislation on healthcare reform\", \"Politics\"),\n",
    "        (\"Presidential election campaign enters final phase with debates\", \"Politics\"),\n",
    "        (\"Supreme Court ruling affects voting rights across the nation\", \"Politics\"),\n",
    "        (\"International summit focuses on climate change policies\", \"Politics\"),\n",
    "        (\"Senate committee investigates government spending on infrastructure\", \"Politics\"),\n",
    "        \n",
    "        # Business articles\n",
    "        (\"Stock market reaches new highs amid economic recovery\", \"Business\"),\n",
    "        (\"Major corporation announces merger with international partner\", \"Business\"),\n",
    "        (\"Cryptocurrency prices fluctuate following regulatory announcements\", \"Business\"),\n",
    "        (\"Retail sales increase during holiday shopping season\", \"Business\"),\n",
    "        (\"Oil prices rise due to supply chain disruptions\", \"Business\"),\n",
    "    ]\n",
    "    \n",
    "    texts, labels = zip(*sample_data)\n",
    "    return list(texts), list(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937816d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to demonstrate the news classifier\"\"\"\n",
    "    print(\"News Article Classification with PyTorch\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create sample data\n",
    "    texts, labels = create_sample_data()\n",
    "    print(f\"Created {len(texts)} sample articles across {len(set(labels))} categories\")\n",
    "    \n",
    "    # Initialize classifier\n",
    "    classifier = NewsClassifier(embed_dim=64, hidden_dim=128, max_length=256)\n",
    "    \n",
    "    # Split data\n",
    "    train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "        texts, labels, test_size=0.3, random_state=42, stratify=labels\n",
    "    )\n",
    "    \n",
    "    # Build vocabulary and prepare labels\n",
    "    classifier.build_vocab(train_texts, min_freq=1)\n",
    "    train_labels_num = classifier.prepare_labels(train_labels)\n",
    "    test_labels_num = [classifier.label_to_idx[label] for label in test_labels]\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader, test_loader = classifier.create_data_loaders(\n",
    "        train_texts, train_labels_num, test_texts, test_labels_num, batch_size=4\n",
    "    )\n",
    "    \n",
    "    # Build and train model\n",
    "    num_classes = len(classifier.label_to_idx)\n",
    "    classifier.build_model(num_classes)\n",
    "    classifier.train(train_loader, test_loader, num_epochs=15, learning_rate=0.001)\n",
    "    \n",
    "    # Make predictions on new text\n",
    "    new_articles = [\n",
    "        \"New smartphone features artificial intelligence and machine learning capabilities\",\n",
    "        \"Football team scores winning touchdown in championship game\",\n",
    "        \"Stock market experiences volatility due to economic uncertainty\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nPredictions on new articles:\")\n",
    "    print(\"-\" * 30)\n",
    "    predictions = classifier.predict(new_articles)\n",
    "    for article, prediction in zip(new_articles, predictions):\n",
    "        print(f\"Article: {article[:60]}...\")\n",
    "        print(f\"Predicted Category: {prediction}\\n\")\n",
    "    \n",
    "    # Save the model\n",
    "    classifier.save_model('news_classifier_model.pth')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f0c732",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
