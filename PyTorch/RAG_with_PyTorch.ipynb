{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f41124cd",
   "metadata": {},
   "source": [
    "# RAG with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4464d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/ajborbz/.pyenv/versions/3.11.9/lib/python3.11/site-packages (1.26.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting torch==2.2.0\n",
      "  Downloading torch-2.2.0-cp311-none-macosx_11_0_arm64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: filelock in /Users/ajborbz/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from torch==2.2.0) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/ajborbz/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from torch==2.2.0) (4.14.0)\n",
      "Requirement already satisfied: sympy in /Users/ajborbz/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from torch==2.2.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/ajborbz/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from torch==2.2.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Users/ajborbz/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from torch==2.2.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/ajborbz/.local/lib/python3.11/site-packages (from torch==2.2.0) (2024.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ajborbz/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from jinja2->torch==2.2.0) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/ajborbz/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from sympy->torch==2.2.0) (1.3.0)\n",
      "Downloading torch-2.2.0-cp311-none-macosx_11_0_arm64.whl (59.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.18.1 requires torch==2.3.1, but you have torch 2.2.0 which is incompatible.\n",
      "torchaudio 2.7.1 requires torch==2.7.1, but you have torch 2.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-2.2.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting sacremoses==0.1.1\n",
      "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: regex in /Users/ajborbz/.local/lib/python3.11/site-packages (from sacremoses==0.1.1) (2024.11.6)\n",
      "Requirement already satisfied: click in /Users/ajborbz/.local/lib/python3.11/site-packages (from sacremoses==0.1.1) (8.2.1)\n",
      "Requirement already satisfied: joblib in /Users/ajborbz/.local/lib/python3.11/site-packages (from sacremoses==0.1.1) (1.5.1)\n",
      "Requirement already satisfied: tqdm in /Users/ajborbz/.local/lib/python3.11/site-packages (from sacremoses==0.1.1) (4.66.4)\n",
      "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sacremoses\n",
      "Successfully installed sacremoses-0.1.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting torchtext\n",
      "  Downloading torchtext-0.18.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: tqdm in /Users/ajborbz/.local/lib/python3.11/site-packages (from torchtext) (4.66.4)\n",
      "Requirement already satisfied: requests in /Users/ajborbz/.local/lib/python3.11/site-packages (from torchtext) (2.32.4)\n",
      "Collecting torch>=2.3.0 (from torchtext)\n",
      "  Using cached torch-2.7.1-cp311-none-macosx_11_0_arm64.whl.metadata (29 kB)\n",
      "Requirement already satisfied: numpy in /Users/ajborbz/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from torchtext) (1.26.0)\n",
      "Requirement already satisfied: filelock in /Users/ajborbz/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from torch>=2.3.0->torchtext) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/ajborbz/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from torch>=2.3.0->torchtext) (4.14.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/ajborbz/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from torch>=2.3.0->torchtext) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/ajborbz/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from torch>=2.3.0->torchtext) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Users/ajborbz/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from torch>=2.3.0->torchtext) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/ajborbz/.local/lib/python3.11/site-packages (from torch>=2.3.0->torchtext) (2024.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/ajborbz/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=2.3.0->torchtext) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ajborbz/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from jinja2->torch>=2.3.0->torchtext) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/ajborbz/.local/lib/python3.11/site-packages (from requests->torchtext) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ajborbz/.local/lib/python3.11/site-packages (from requests->torchtext) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ajborbz/.local/lib/python3.11/site-packages (from requests->torchtext) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ajborbz/.local/lib/python3.11/site-packages (from requests->torchtext) (2025.7.14)\n",
      "Downloading torchtext-0.18.0-cp311-cp311-macosx_11_0_arm64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m-:--:--\u001b[0m\n",
      "\u001b[?25hUsing cached torch-2.7.1-cp311-none-macosx_11_0_arm64.whl (68.6 MB)\n",
      "Installing collected packages: torch, torchtext\n",
      "\u001b[2K  Attempting uninstall: torch\n",
      "\u001b[2K    Found existing installation: torch 2.2.0\n",
      "\u001b[2K    Uninstalling torch-2.2.0:\n",
      "\u001b[2K      Successfully uninstalled torch-2.2.0━━━━━━\u001b[0m \u001b[32m0/2\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [torchtext]/2\u001b[0m [torchtext]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.18.1 requires torch==2.3.1, but you have torch 2.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-2.7.1 torchtext-0.18.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting transformers==4.40.2\n",
      "  Downloading transformers-4.40.2-py3-none-any.whl.metadata (137 kB)\n",
      "Requirement already satisfied: filelock in /Users/ajborbz/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from transformers==4.40.2) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/ajborbz/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from transformers==4.40.2) (0.33.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/ajborbz/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from transformers==4.40.2) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ajborbz/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from transformers==4.40.2) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/ajborbz/.local/lib/python3.11/site-packages (from transformers==4.40.2) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/ajborbz/.local/lib/python3.11/site-packages (from transformers==4.40.2) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/ajborbz/.local/lib/python3.11/site-packages (from transformers==4.40.2) (2.32.4)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers==4.40.2)\n",
      "  Using cached tokenizers-0.19.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/ajborbz/.local/lib/python3.11/site-packages (from transformers==4.40.2) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/ajborbz/.local/lib/python3.11/site-packages (from transformers==4.40.2) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/ajborbz/.local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.2) (2024.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ajborbz/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.2) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /Users/ajborbz/.local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.2) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/ajborbz/.local/lib/python3.11/site-packages (from requests->transformers==4.40.2) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ajborbz/.local/lib/python3.11/site-packages (from requests->transformers==4.40.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ajborbz/.local/lib/python3.11/site-packages (from requests->transformers==4.40.2) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ajborbz/.local/lib/python3.11/site-packages (from requests->transformers==4.40.2) (2025.7.14)\n",
      "Downloading transformers-4.40.2-py3-none-any.whl (9.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tokenizers-0.19.1-cp311-cp311-macosx_11_0_arm64.whl (2.4 MB)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [transformers][0m [transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed tokenizers-0.19.1 transformers-4.40.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting matplotlib==3.8.4\n",
      "  Downloading matplotlib-3.8.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/ajborbz/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from matplotlib==3.8.4) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/ajborbz/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from matplotlib==3.8.4) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/ajborbz/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from matplotlib==3.8.4) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/ajborbz/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from matplotlib==3.8.4) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.21 in /Users/ajborbz/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from matplotlib==3.8.4) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ajborbz/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from matplotlib==3.8.4) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /Users/ajborbz/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from matplotlib==3.8.4) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/ajborbz/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from matplotlib==3.8.4) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/ajborbz/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from matplotlib==3.8.4) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ajborbz/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib==3.8.4) (1.17.0)\n",
      "Downloading matplotlib-3.8.4-cp311-cp311-macosx_11_0_arm64.whl (7.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: matplotlib\n",
      "Successfully installed matplotlib-3.8.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting sentencepiece==0.2.0\n",
      "  Downloading sentencepiece-0.2.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Downloading sentencepiece-0.2.0-cp311-cp311-macosx_11_0_arm64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m-:--:--\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting scikit-learn==1.4.2\n",
      "  Downloading scikit_learn-1.4.2-cp311-cp311-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Users/ajborbz/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from scikit-learn==1.4.2) (1.26.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/ajborbz/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from scikit-learn==1.4.2) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/ajborbz/.local/lib/python3.11/site-packages (from scikit-learn==1.4.2) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/ajborbz/.local/lib/python3.11/site-packages (from scikit-learn==1.4.2) (3.6.0)\n",
      "Downloading scikit_learn-1.4.2-cp311-cp311-macosx_12_0_arm64.whl (10.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: scikit-learn\n",
      "Successfully installed scikit-learn-1.4.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --user numpy\n",
    "!pip install --user torch==2.2.0\n",
    "!pip install --user sacremoses==0.1.1\n",
    "!pip install --user torchtext\n",
    "!pip install --user transformers==4.40.2\n",
    "!pip install --user matplotlib==3.8.4\n",
    "!pip install --user sentencepiece==0.2.0\n",
    "!pip install --user scikit-learn==1.4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efcf075f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abf947f",
   "metadata": {},
   "source": [
    "### Define Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec4babf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_plot(data, plot):\n",
    "    # Apply t-SNE to reduce to 3D\n",
    "    tsne = TSNE(n_components=3, random_state=42, perplexity=min(50, data.shape[0] - 1))  # Using 50 or less based on data size\n",
    "    data_3d = tsne.fit_transform(data)\n",
    "    \n",
    "    # Plotting\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Assign colors for each point based on its index\n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, len(data_3d)))\n",
    "    for idx, point in zip(range(len(data_3d)), data_3d):\n",
    "        ax.scatter(point[0], point[1], point[2], color=colors[idx], label=f'{plot} {idx+1}')\n",
    "    \n",
    "    # Adding labels and titles\n",
    "    ax.set_xlabel('TSNE Component 1')\n",
    "    ax.set_ylabel('TSNE Component 2')\n",
    "    ax.set_zlabel('TSNE Component 3')\n",
    "    plt.title('3D t-SNE Visualization of '+ plot +' Embeddings')\n",
    "    plt.legend(title=plot +' Index', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d49b605",
   "metadata": {},
   "source": [
    "# Embeddings using BERT and PyTorch Hub\n",
    "\n",
    "Use PyTorch and the Transformers library by Hugging Face to tokenize text, convert it to embeddings using BERT, and handle these embeddings with a model.\n",
    "\n",
    "## Loading tokenizer and model \n",
    "Let's begin by loading a tokenizer and later a model, both specifically bert-base-uncased. This is done using torch.hub.load, which is a convenient way to load pre-trained models and tokenizers directly from Hugging Face's model hub.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5c18ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ac514b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input text to get embeddings for\n",
    "input_text = [(\"This is an example sentence for BERT embeddings.\", \"How do you like it \"),(\"There are other models\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd29c04",
   "metadata": {},
   "source": [
    "#### Tokenization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f2eea03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2023, 2003, 2019, 2742, 6251, 2005, 14324, 7861, 8270, 4667, 2015, 1012, 102, 2129, 2079, 2017, 2066, 2009, 102], [101, 2045, 2024, 2060, 4275, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.batch_encode_plus(input_text,add_special_tokens=True,padding=True,truncation=True)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7558a57d",
   "metadata": {},
   "source": [
    "### Text decoding and verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2de0dfb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] this is an example sentence for bert embeddings. [SEP] how do you like it [SEP]\n",
      "length 16\n"
     ]
    }
   ],
   "source": [
    "text=tokenizer.decode(input_ids['input_ids'][0])\n",
    "print(text)\n",
    "print(f\"length {len(text.split())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4f013f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids['attention_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6663ccbc",
   "metadata": {},
   "source": [
    "\n",
    "When using the BERT tokenizer, the output includes key components that are essential for the model's processing:\n",
    "\n",
    "```input_ids```: A list of token IDs that represent each token in BERT's vocabulary.\n",
    "\n",
    "```token_type_ids```: Indicates which sentence each token belongs to, important for tasks involving sentence pairs.\n",
    "\n",
    "```attention_mask```: Identifies which tokens should be focused on, differentiating real content from padding.\n",
    "\n",
    "\n",
    "Special tokens:\n",
    "\n",
    "[CLS]: Placed at the start of every input for use in classification tasks.\n",
    "\n",
    "[SEP]: Separates sentences in dual-sentence tasks and marks the end of input sequences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d251b09c",
   "metadata": {},
   "source": [
    "### Device converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a782d18a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13adbaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_tensors = torch.tensor(input_ids['input_ids']).to(DEVICE)\n",
    "mask_tensors = torch.tensor(input_ids['attention_mask']).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1224d24",
   "metadata": {},
   "source": [
    "### Load the BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bec41e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bda0d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04a9c87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embding=bert_model(input_ids_tensors,mask_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdd46a6",
   "metadata": {},
   "source": [
    "### Generating aggregated mean embeddings using BERT for RAG\n",
    "Here, you'll compute aggregated mean embeddings for input sequences using the BERT model you just loaded. It processes each pair of token IDs and attention masks from the input data, extracts word embeddings for non-padded tokens, and calculates their mean. The result is a list of mean embeddings for each sequence, which is then concatenated into a single tensor. This process allows for the generation of simplified yet informative representations of the input sequences, useful for tasks like clustering, similarity search, or as input to downstream models. Each document must be under 512 tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee413f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00, 17.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_ids_tensor shape: torch.Size([1, 20]) torch.Size([1, 20])\n",
      "Word embeddings shape: torch.Size([20, 768])\n",
      "Number of zero padding embeddings: 0\n",
      "valid_embeddings_mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True])\n",
      "Word embeddings after zero padding embeddings removed: torch.Size([20, 768])\n",
      "Mean embedding shape: torch.Size([768])\n",
      "token_ids_tensor shape: torch.Size([1, 20]) torch.Size([1, 20])\n",
      "Word embeddings shape: torch.Size([20, 768])\n",
      "Number of zero padding embeddings: 14\n",
      "valid_embeddings_mask: tensor([ True,  True,  True,  True,  True,  True, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False])\n",
      "Word embeddings after zero padding embeddings removed: torch.Size([6, 768])\n",
      "Mean embedding shape: torch.Size([768])\n",
      "All mean embeddings shape: torch.Size([2, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize a list to store the mean embeddings for each input sequence\n",
    "aggregated_mean_embeddings = []\n",
    "\n",
    "# Loop over each pair of input_ids and attention_masks\n",
    "for token_ids, attention_mask in tqdm(zip(input_ids['input_ids'], input_ids['attention_mask'])):\n",
    "    # Convert list of token ids and attention mask to tensors\n",
    "    token_ids_tensor = torch.tensor([token_ids]).to(DEVICE)\n",
    "    attention_mask_tensor = torch.tensor([attention_mask]).to(DEVICE)\n",
    "    print(\"token_ids_tensor shape:\",token_ids_tensor.shape, attention_mask_tensor.shape)  # Print the shapes of the input tensors\n",
    "    with torch.no_grad():  # Disable gradient calculations for faster execution\n",
    "        # Retrieve the batch of word embeddings from the BERT model\n",
    "        embeddings = bert_model(token_ids_tensor, attention_mask=attention_mask_tensor)[0].squeeze(0)\n",
    "        print(\"Word embeddings shape:\", embeddings.shape)\n",
    "        \n",
    "        # Count and print the number of zero-padding embeddings\n",
    "        num_zero_paddings = (attention_mask_tensor == 0).sum().item()\n",
    "        print(\"Number of zero padding embeddings:\", num_zero_paddings)\n",
    "        \n",
    "        # Create a mask for positions that are not zero-padded\n",
    "        valid_embeddings_mask = attention_mask_tensor[0] != 0\n",
    "        print(\"valid_embeddings_mask:\",valid_embeddings_mask)\n",
    "        \n",
    "        # Filter out the embeddings corresponding to zero-padded positions\n",
    "        filtered_embeddings = embeddings[valid_embeddings_mask, :]\n",
    "        print(\"Word embeddings after zero padding embeddings removed:\", filtered_embeddings.shape)\n",
    "        \n",
    "        # Compute the mean of the filtered embeddings\n",
    "        mean_embedding = filtered_embeddings.mean(axis=0)\n",
    "        print(\"Mean embedding shape:\", mean_embedding.shape)\n",
    "    \n",
    "        # Append the mean embedding to the list, adding a batch dimension\n",
    "        aggregated_mean_embeddings.append(mean_embedding.unsqueeze(0))\n",
    "\n",
    "# Concatenate all mean embeddings to form a single tensor\n",
    "aggregated_mean_embeddings = torch.cat(aggregated_mean_embeddings)\n",
    "print('All mean embeddings shape:', aggregated_mean_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0bd52a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_embeddings(input_ids, attention_masks, bert_model=bert_model):\n",
    "    \"\"\"\n",
    "    Converts token indices and masks to word embeddings, filters out zero-padded embeddings,\n",
    "    and aggregates them by computing the mean embedding for each input sequence.\n",
    "\n",
    "    \"\"\"\n",
    "    mean_embeddings = []\n",
    "    # Process each sequence in the batch\n",
    "    print('number of inputs',len(input_ids))\n",
    "    for input_id, mask in tqdm(zip(input_ids, attention_masks)):\n",
    "        input_ids_tensor = torch.tensor([input_id]).to(DEVICE)\n",
    "        mask_tensor = torch.tensor([mask]).to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Obtain the word embeddings from the BERT model\n",
    "            word_embeddings = bert_model(input_ids_tensor, attention_mask=mask_tensor)[0].squeeze(0)\n",
    "\n",
    "            # Filter out the embeddings at positions where the mask is zero \n",
    "            valid_embeddings_mask=mask_tensor[0] != 0 \n",
    "            valid_embeddings = word_embeddings[valid_embeddings_mask,:]\n",
    "            # Compute the mean of the filtered embeddings\n",
    "            mean_embedding = valid_embeddings.mean(dim=0)\n",
    "            mean_embeddings.append(mean_embedding.unsqueeze(0))\n",
    "\n",
    "    # Concatenate the mean embeddings from all sequences in the batch\n",
    "    aggregated_mean_embeddings = torch.cat(mean_embeddings)\n",
    "    return aggregated_mean_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984ce87d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
