{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "430e7714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Increase recursion limit to prevent potential issues\n",
    "sys.setrecursionlimit(100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97410c3a",
   "metadata": {},
   "source": [
    "The sys.setrecursionlimit function is used to increase the recursion limit, which helps prevent potential recursion errors when running complex models with deep nested functions or when using certain libraries like TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4f9ee2",
   "metadata": {},
   "source": [
    "- **`keras_tuner`**: Used for hyperparameter tuning.\n",
    "- **`Sequential`**: A linear stack of layers in Keras.\n",
    "- **`Dense`**, **`Flatten`**: Common Keras layers.\n",
    "- **`mnist`**: The MNIST dataset, a standard dataset for image classification.\n",
    "- **`Adam`**: An optimizer in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0d5f638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Suppress all Python warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set TensorFlow log level to suppress warnings and info messages\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # 0 = all logs, 1 = filter out INFO, 2 = filter out INFO and WARNING, 3 = ERROR only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac21061",
   "metadata": {},
   "source": [
    "- **`mnist.load_data()`**: Loads the dataset, returning training and validation splits.\n",
    "- **`x_train / 255.0`**: Normalizes the pixel values to be between 0 and 1.\n",
    "- **`print(f'...')`**: Displays the shapes of the training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "153c8b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (60000, 28, 28)\n",
      "Validation data shape: (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_val, y_val) = mnist.load_data()\n",
    "x_train, x_val = x_train / 255.0, x_val / 255.0\n",
    "\n",
    "print(f'Training data shape: {x_train.shape}')\n",
    "print(f'Validation data shape: {x_val.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab539a2",
   "metadata": {},
   "source": [
    "### Define the model with Hyperparameters\n",
    "**Define a model-building function:**\n",
    "- Create a function `build_model` that takes a `HyperParameters` object as input.\n",
    "- Use the `HyperParameters` object to define the number of units in a dense layer and the learning rate for the optimizer.\n",
    "- Compile the model with sparse categorical cross-entropy loss and Adam optimizer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc505d8",
   "metadata": {},
   "source": [
    "- **`hp.Int('units', ...)`**: Defines the number of units in the Dense layer as a hyperparameter.\n",
    "- **`hp.Float('learning_rate', ...)`**: Defines the learning rate as a hyperparameter.\n",
    "- **`model.compile()`**: Compiles the model with the Adam optimizer and sparse categorical cross-entropy loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca103f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a model-building function \n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=(28, 28)),\n",
    "        Dense(units=hp.Int('units', min_value=32, max_value=512, step=32), activation='relu'),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ad05bf",
   "metadata": {},
   "source": [
    "### Create a RandomSearch Tuner:\n",
    "\n",
    "Use the RandomSearch class from Keras Tuner.\n",
    "Specify the model-building function, optimization objective (validation accuracy), number of trials, and directory for storing results.\n",
    "\n",
    "- **`build_model`**: The model-building function.\n",
    "- **`objective='val_accuracy'`**: The metric to optimize (validation accuracy).\n",
    "- **`max_trials=10`**: The maximum number of different hyperparameter configurations to try.\n",
    "- **`executions_per_trial=2`**: The number of times to run each configuration.\n",
    "- **`directory='my_dir'`**: Directory to save the results.\n",
    "- **`project_name='intro_to_kt'`**: Name of the project for organizing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a10f25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 2\n",
      "units (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
      "learning_rate (Float)\n",
      "{'default': 0.0001, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n"
     ]
    }
   ],
   "source": [
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=2,\n",
    "    directory='my_dir',\n",
    "    project_name='intro_to_kt'\n",
    ")\n",
    "\n",
    "# Display a summary of the search space \n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12c1d15",
   "metadata": {},
   "source": [
    "- **`epochs=5`**: Each trial is trained for 5 epochs.\n",
    "- **`validation_data=(x_val, y_val)`**: The validation data to evaluate the model's performance during the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21c61efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 00m 14s]\n",
      "val_accuracy: 0.9611499905586243\n",
      "\n",
      "Best val_accuracy So Far: 0.9784500002861023\n",
      "Total elapsed time: 00h 05m 19s\n",
      "Results summary\n",
      "Results in my_dir/intro_to_kt\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_accuracy\", direction=\"max\")\n",
      "\n",
      "Trial 08 summary\n",
      "Hyperparameters:\n",
      "units: 224\n",
      "learning_rate: 0.0007784635441963913\n",
      "Score: 0.9784500002861023\n",
      "\n",
      "Trial 07 summary\n",
      "Hyperparameters:\n",
      "units: 256\n",
      "learning_rate: 0.0024479132350219944\n",
      "Score: 0.9767000079154968\n",
      "\n",
      "Trial 02 summary\n",
      "Hyperparameters:\n",
      "units: 416\n",
      "learning_rate: 0.0040812671360801476\n",
      "Score: 0.9766499996185303\n",
      "\n",
      "Trial 01 summary\n",
      "Hyperparameters:\n",
      "units: 128\n",
      "learning_rate: 0.0012179247857147329\n",
      "Score: 0.9764499962329865\n",
      "\n",
      "Trial 05 summary\n",
      "Hyperparameters:\n",
      "units: 416\n",
      "learning_rate: 0.003863170317069186\n",
      "Score: 0.9727999866008759\n",
      "\n",
      "Trial 04 summary\n",
      "Hyperparameters:\n",
      "units: 384\n",
      "learning_rate: 0.0001383021456134251\n",
      "Score: 0.9700500071048737\n",
      "\n",
      "Trial 03 summary\n",
      "Hyperparameters:\n",
      "units: 480\n",
      "learning_rate: 0.006822989239763269\n",
      "Score: 0.9688500165939331\n",
      "\n",
      "Trial 06 summary\n",
      "Hyperparameters:\n",
      "units: 512\n",
      "learning_rate: 0.00701395826845125\n",
      "Score: 0.9674500226974487\n",
      "\n",
      "Trial 09 summary\n",
      "Hyperparameters:\n",
      "units: 32\n",
      "learning_rate: 0.00707357913209643\n",
      "Score: 0.9611499905586243\n",
      "\n",
      "Trial 00 summary\n",
      "Hyperparameters:\n",
      "units: 32\n",
      "learning_rate: 0.00682729045758731\n",
      "Score: 0.9554499983787537\n"
     ]
    }
   ],
   "source": [
    "# Run the hyperparameter search \n",
    "tuner.search(x_train, y_train, epochs=5, validation_data=(x_val, y_val)) \n",
    "\n",
    "# Display a summary of the results \n",
    "tuner.results_summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "080a2c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "The optimal number of units in the first dense layer is 224. \n",
      "\n",
      "The optimal learning rate for the optimizer is 0.0007784635441963913. \n",
      "\n",
      "\n",
      "Epoch 1/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8671 - loss: 0.4725 - val_accuracy: 0.9574 - val_loss: 0.1532\n",
      "Epoch 2/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9638 - loss: 0.1250 - val_accuracy: 0.9661 - val_loss: 0.1114\n",
      "Epoch 3/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9769 - loss: 0.0788 - val_accuracy: 0.9712 - val_loss: 0.0955\n",
      "Epoch 4/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9842 - loss: 0.0549 - val_accuracy: 0.9741 - val_loss: 0.0856\n",
      "Epoch 5/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9880 - loss: 0.0403 - val_accuracy: 0.9722 - val_loss: 0.0877\n",
      "Epoch 6/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9916 - loss: 0.0295 - val_accuracy: 0.9771 - val_loss: 0.0821\n",
      "Epoch 7/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9942 - loss: 0.0204 - val_accuracy: 0.9754 - val_loss: 0.0865\n",
      "Epoch 8/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9945 - loss: 0.0189 - val_accuracy: 0.9744 - val_loss: 0.0934\n",
      "Epoch 9/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9970 - loss: 0.0124 - val_accuracy: 0.9787 - val_loss: 0.0829\n",
      "Epoch 10/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9971 - loss: 0.0106 - val_accuracy: 0.9744 - val_loss: 0.0931\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 632us/step - accuracy: 0.9713 - loss: 0.1090\n",
      "Test accuracy: 0.9764999747276306\n"
     ]
    }
   ],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0] \n",
    "print(f\"\"\" \n",
    "\n",
    "The optimal number of units in the first dense layer is {best_hps.get('units')}. \n",
    "\n",
    "The optimal learning rate for the optimizer is {best_hps.get('learning_rate')}. \n",
    "\n",
    "\"\"\") \n",
    "\n",
    "# Step 2: Build and Train the Model with Best Hyperparameters \n",
    "model = tuner.hypermodel.build(best_hps) \n",
    "model.fit(x_train, y_train, epochs=10, validation_split=0.2) \n",
    "\n",
    "# Evaluate the model on the test set \n",
    "test_loss, test_acc = model.evaluate(x_val, y_val) \n",
    "print(f'Test accuracy: {test_acc}') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5379d7e6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
