{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd2c143d",
   "metadata": {},
   "source": [
    "# **In-Context Engineering and Prompt Templates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5aaa436",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --user \"ibm-watsonx-ai==0.2.6\"\n",
    "!pip install --user \"langchain==0.1.16\" \n",
    "!pip install --user \"langchain-ibm==0.1.4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44ff016d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from ibm_watsonx_ai.foundation_models import Model\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes\n",
    "from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7136a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_model(prompt_txt, params=None):\n",
    "    model_id = 'mistralai/mixtral-8x7b-instruct-v01'\n",
    "\n",
    "    default_params = {\n",
    "        \"max_new_tokens\": 256,\n",
    "        \"min_new_tokens\": 0,\n",
    "        \"temperature\": 0.5,\n",
    "        \"top_p\": 0.2,\n",
    "        \"top_k\": 1\n",
    "    }\n",
    "\n",
    "    if params:\n",
    "        default_params.update(params)\n",
    "\n",
    "    parameters = {\n",
    "        GenParams.MAX_NEW_TOKENS: default_params[\"max_new_tokens\"],  # this controls the maximum number of tokens in the generated output\n",
    "        GenParams.MIN_NEW_TOKENS: default_params[\"min_new_tokens\"], # this controls the minimum number of tokens in the generated output\n",
    "        GenParams.TEMPERATURE: default_params[\"temperature\"], # this randomness or creativity of the model's responses\n",
    "        GenParams.TOP_P: default_params[\"top_p\"],\n",
    "        GenParams.TOP_K: default_params[\"top_k\"]\n",
    "    }\n",
    "    \n",
    "    credentials = {\n",
    "        \"url\": \"https://us-south.ml.cloud.ibm.com\"\n",
    "    }\n",
    "    \n",
    "    project_id = \"skills-network\"\n",
    "    \n",
    "    model = Model(\n",
    "        model_id=model_id,\n",
    "        params=parameters,\n",
    "        credentials=credentials,\n",
    "        project_id=project_id\n",
    "    )\n",
    "    \n",
    "    mixtral_llm = WatsonxLLM(model=model)\n",
    "    response  = mixtral_llm.invoke(prompt_txt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a68e08ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decoding_method': 'sample',\n",
       " 'length_penalty': {'decay_factor': 2.5, 'start_index': 5},\n",
       " 'temperature': 0.5,\n",
       " 'top_p': 0.2,\n",
       " 'top_k': 1,\n",
       " 'random_seed': 33,\n",
       " 'repetition_penalty': 2,\n",
       " 'min_new_tokens': 50,\n",
       " 'max_new_tokens': 200,\n",
       " 'stop_sequences': ['fail'],\n",
       " ' time_limit': 600000,\n",
       " 'truncate_input_tokens': 200,\n",
       " 'return_options': {'input_text': True,\n",
       "  'generated_tokens': True,\n",
       "  'input_tokens': True,\n",
       "  'token_logprobs': True,\n",
       "  'token_ranks': False,\n",
       "  'top_n_tokens': False}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GenParams().get_example_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f13544",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
