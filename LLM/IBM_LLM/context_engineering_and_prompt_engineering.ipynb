{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd2c143d",
   "metadata": {},
   "source": [
    "# **In-Context Engineering and Prompt Templates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5aaa436",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --user \"ibm-watsonx-ai==0.2.6\"\n",
    "!pip install --user \"langchain==0.1.16\" \n",
    "!pip install --user \"langchain-ibm==0.1.4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44ff016d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from ibm_watsonx_ai.foundation_models import Model\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes\n",
    "from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7136a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_model(prompt_txt, params=None):\n",
    "    model_id = 'mistralai/mixtral-8x7b-instruct-v01'\n",
    "\n",
    "    default_params = {\n",
    "        \"max_new_tokens\": 256,\n",
    "        \"min_new_tokens\": 0,\n",
    "        \"temperature\": 0.5,\n",
    "        \"top_p\": 0.2,\n",
    "        \"top_k\": 1\n",
    "    }\n",
    "\n",
    "    if params:\n",
    "        default_params.update(params)\n",
    "\n",
    "    parameters = {\n",
    "        GenParams.MAX_NEW_TOKENS: default_params[\"max_new_tokens\"],  # this controls the maximum number of tokens in the generated output\n",
    "        GenParams.MIN_NEW_TOKENS: default_params[\"min_new_tokens\"], # this controls the minimum number of tokens in the generated output\n",
    "        GenParams.TEMPERATURE: default_params[\"temperature\"], # this randomness or creativity of the model's responses\n",
    "        GenParams.TOP_P: default_params[\"top_p\"],\n",
    "        GenParams.TOP_K: default_params[\"top_k\"]\n",
    "    }\n",
    "    \n",
    "    credentials = {\n",
    "        \"url\": \"https://us-south.ml.cloud.ibm.com\"\n",
    "    }\n",
    "    \n",
    "    project_id = \"skills-network\"\n",
    "    \n",
    "    model = Model(\n",
    "        model_id=model_id,\n",
    "        params=parameters,\n",
    "        credentials=credentials,\n",
    "        project_id=project_id\n",
    "    )\n",
    "    \n",
    "    mixtral_llm = WatsonxLLM(model=model)\n",
    "    response  = mixtral_llm.invoke(prompt_txt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a68e08ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decoding_method': 'sample',\n",
       " 'length_penalty': {'decay_factor': 2.5, 'start_index': 5},\n",
       " 'temperature': 0.5,\n",
       " 'top_p': 0.2,\n",
       " 'top_k': 1,\n",
       " 'random_seed': 33,\n",
       " 'repetition_penalty': 2,\n",
       " 'min_new_tokens': 50,\n",
       " 'max_new_tokens': 200,\n",
       " 'stop_sequences': ['fail'],\n",
       " ' time_limit': 600000,\n",
       " 'truncate_input_tokens': 200,\n",
       " 'return_options': {'input_text': True,\n",
       "  'generated_tokens': True,\n",
       "  'input_tokens': True,\n",
       "  'token_logprobs': True,\n",
       "  'token_ranks': False,\n",
       "  'top_n_tokens': False}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GenParams().get_example_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de94555d",
   "metadata": {},
   "source": [
    "### Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e3422f",
   "metadata": {},
   "source": [
    "In this example, let's introduce a basic prompt that utilizes specific parameters to guide the language model's response. You'll then define a simple prompt and retrieve the model's response,\n",
    "\n",
    "The prompt used is \"The wind is\". Let the model generate itself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f13544",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"max_new_tokens\": 128,\n",
    "    \"min_new_tokens\": 10,\n",
    "    \"temperature\": 0.5,\n",
    "    \"top_p\": 0.2,\n",
    "    \"top_k\": 1\n",
    "}\n",
    "\n",
    "prompt = \"The wind is\"\n",
    "\n",
    "response = llm_model(prompt, params)\n",
    "print(f\"prompt: {prompt}\\n\")\n",
    "print(f\"response : {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c6f668",
   "metadata": {},
   "source": [
    "As you can see from the response, the model continues generating content following the initial prompt, \"The wind is\". You might notice that the response appears truncated or incomplete. This is because you have set the `max_new_tokens,` which restricts the number of tokens the model can generate.\n",
    "\n",
    "Try to adjust the parameters and observe the difference in the response.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fdb172",
   "metadata": {},
   "source": [
    "### Zero-shot prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edec11f5",
   "metadata": {},
   "source": [
    "Here is an example of a zero-shot prompt. \n",
    "\n",
    "Zero-shot learning is crucial for testing a model's ability to apply its pre-trained knowledge to new, unseen tasks without additional training. This capability is valuable for gauging the model's generalization skills.\n",
    "\n",
    "In this example, let's demonstrate a zero-shot learning scenario using a prompt that asks the model to classify a statement without any prior specific training on similar tasks. The prompt requests the model to assess the truthfulness of the statement: \"The Eiffel Tower is located in Berlin.\". After defining the prompt, you'll execute it with default parameters and print the response.\n",
    "\n",
    "This approach helps you understand how well the model can handle direct questions based on its underlying knowledge and reasoning abilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0efd533",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Classify the following statement as true or false: \n",
    "            'The Eiffel Tower is located in Berlin.'\n",
    "\n",
    "            Answer:\n",
    "\"\"\"\n",
    "response = llm_model(prompt, params)\n",
    "print(f\"prompt: {prompt}\\n\")\n",
    "print(f\"response : {response}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
